{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF5wCw1ISWfMjLitTTJuir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyesh-Kanagavel/deep_learning--keras/blob/master/GPT_tokenizer_Andrej_Karpathy_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization : a very important part of nlp applications. an independent pipeline which has its own training needs, bytecode implementations etc\n"
      ],
      "metadata": {
        "id": "8yR6ffZZ7wCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the goal is to get to a state where there are no tokenizations required and the transformer just works on byte streams of data. There is already a paper published ,but not yet proved at scale."
      ],
      "metadata": {
        "id": "qq4bZ7byV_i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the unicode standard is used to encode text in python and many other programmign languanges. in python, for a given character, ord function is used to retrieve the unicode."
      ],
      "metadata": {
        "id": "eegW45qW7K0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord(\"h\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtWvWfxp75Y5",
        "outputId": "809a0107-849e-4bb1-8ab5-bec4a0cc9ac6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even emojis like hi , laughing emojis are given unicode integer code."
      ],
      "metadata": {
        "id": "p1Nhutai7lZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord(\"üòÇ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agjwllxA7XPX",
        "outputId": "c65d391a-99ee-48b7-9b65-16f8ee7a7c90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128514"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\"üëãüèª , hi as a emoji\").encode(\"utf-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSZgMdFC7kAD",
        "outputId": "2394cad5-e698-453f-bb54-44152feb522b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\xf0\\x9f\\x91\\x8b\\xf0\\x9f\\x8f\\xbb , hi as a emoji'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with encode(\"utf-8\") method, the output is a sequence of non-readable bytes shown in hex format. if it is read as list, it gives a neat sequence of integers.\n"
      ],
      "metadata": {
        "id": "uWOTquiC9R2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: l\n",
        "\n",
        "l = list((\"üëãüèª , hi as a emoji\").encode(\"utf-8\"))\n",
        "print(l)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDj_FXYl9QWJ",
        "outputId": "8917c84a-9f2e-4398-e9c3-7fcfe3c4df0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[240, 159, 145, 139, 240, 159, 143, 187, 32, 44, 32, 104, 105, 32, 97, 115, 32, 97, 32, 101, 109, 111, 106, 105]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = list((\"üëãüèª , hi as a emoji\").encode(\"utf-16\"))\n",
        "print(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2REi4MQc9lqI",
        "outputId": "935a68b7-e0b7-4bf2-8b39-3d75b17a4e73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[255, 254, 61, 216, 75, 220, 60, 216, 251, 223, 32, 0, 44, 0, 32, 0, 104, 0, 105, 0, 32, 0, 97, 0, 115, 0, 32, 0, 97, 0, 32, 0, 101, 0, 109, 0, 111, 0, 106, 0, 105, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we see that in utf-16 , there are zeros in between, meaning that a lot of bits are wasted compared to utf-8"
      ],
      "metadata": {
        "id": "uQw6JNfc91Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we use UTF-8 raw byte streams as inputs to transformers, it could be the ultimate goal, but it has a problem, for given raw texts, the raw byte encodings will be very long and the transformers will not be able to attend to a very large past or future data.\n",
        "so, algos like bytepair encoding is used. here, the sequence is scanned and if there are particular sub sequences of strings which occur often, they are replaced by another character which is then appended to the vocab.\n",
        "For example:\n",
        "aabcaabdaabc\n",
        "Z = aa\n",
        "ZbcZbdZbc\n",
        "now Y=bc\n",
        "ZYZbdZY\n"
      ],
      "metadata": {
        "id": "jwydpwVL_f4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"You can quickly replace text with emoji suggestions as you type.Enter a commonly used word or phrase like ‚Äúheart‚Äù or ‚Äúthumbs up‚Äù, then press Fn-E or the Globe key-E.If there are no emoji suggestions for the text you entered, the Character Viewer opens.Press Return to replace the text with the suggested emoji, or choose another suggestion.Click the Arrow button  to see additional suggestions and emoji in the Character Viewer.Tip: You can automatically insert an emoji every time you type certain text (for example, automatically insert üòÜ when you type XD). See Replace text and punctuation in documents.\"\n",
        "tokens = text.encode('utf-8')\n",
        "tokens = list(map(int,tokens)) # convert to a list of integers\n",
        "print(\"the length of text is: \", len(text))\n",
        "print(\"the length of tokens is : \", len(tokens))\n",
        "print(\"the list of tokens : \", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRz4LVva9y1z",
        "outputId": "a3badcbb-1b96-4b14-f00b-2b17897997d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the length of text is:  606\n",
            "the length of tokens is :  617\n",
            "the list of tokens :  [89, 111, 117, 32, 99, 97, 110, 32, 113, 117, 105, 99, 107, 108, 121, 32, 114, 101, 112, 108, 97, 99, 101, 32, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 101, 109, 111, 106, 105, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 115, 32, 97, 115, 32, 121, 111, 117, 32, 116, 121, 112, 101, 46, 69, 110, 116, 101, 114, 32, 97, 32, 99, 111, 109, 109, 111, 110, 108, 121, 32, 117, 115, 101, 100, 32, 119, 111, 114, 100, 32, 111, 114, 32, 112, 104, 114, 97, 115, 101, 32, 108, 105, 107, 101, 32, 226, 128, 156, 104, 101, 97, 114, 116, 226, 128, 157, 32, 111, 114, 32, 226, 128, 156, 116, 104, 117, 109, 98, 115, 32, 117, 112, 226, 128, 157, 44, 32, 116, 104, 101, 110, 32, 112, 114, 101, 115, 115, 32, 70, 110, 45, 69, 32, 111, 114, 32, 116, 104, 101, 32, 71, 108, 111, 98, 101, 32, 107, 101, 121, 45, 69, 46, 73, 102, 32, 116, 104, 101, 114, 101, 32, 97, 114, 101, 32, 110, 111, 32, 101, 109, 111, 106, 105, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 115, 32, 102, 111, 114, 32, 116, 104, 101, 32, 116, 101, 120, 116, 32, 121, 111, 117, 32, 101, 110, 116, 101, 114, 101, 100, 44, 32, 116, 104, 101, 32, 67, 104, 97, 114, 97, 99, 116, 101, 114, 32, 86, 105, 101, 119, 101, 114, 32, 111, 112, 101, 110, 115, 46, 80, 114, 101, 115, 115, 32, 82, 101, 116, 117, 114, 110, 32, 116, 111, 32, 114, 101, 112, 108, 97, 99, 101, 32, 116, 104, 101, 32, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 116, 104, 101, 32, 115, 117, 103, 103, 101, 115, 116, 101, 100, 32, 101, 109, 111, 106, 105, 44, 32, 111, 114, 32, 99, 104, 111, 111, 115, 101, 32, 97, 110, 111, 116, 104, 101, 114, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 46, 67, 108, 105, 99, 107, 32, 116, 104, 101, 32, 65, 114, 114, 111, 119, 32, 98, 117, 116, 116, 111, 110, 32, 32, 116, 111, 32, 115, 101, 101, 32, 97, 100, 100, 105, 116, 105, 111, 110, 97, 108, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 115, 32, 97, 110, 100, 32, 101, 109, 111, 106, 105, 32, 105, 110, 32, 116, 104, 101, 32, 67, 104, 97, 114, 97, 99, 116, 101, 114, 32, 86, 105, 101, 119, 101, 114, 46, 84, 105, 112, 58, 32, 89, 111, 117, 32, 99, 97, 110, 32, 97, 117, 116, 111, 109, 97, 116, 105, 99, 97, 108, 108, 121, 32, 105, 110, 115, 101, 114, 116, 32, 97, 110, 32, 101, 109, 111, 106, 105, 32, 101, 118, 101, 114, 121, 32, 116, 105, 109, 101, 32, 121, 111, 117, 32, 116, 121, 112, 101, 32, 99, 101, 114, 116, 97, 105, 110, 32, 116, 101, 120, 116, 32, 40, 102, 111, 114, 32, 101, 120, 97, 109, 112, 108, 101, 44, 32, 97, 117, 116, 111, 109, 97, 116, 105, 99, 97, 108, 108, 121, 32, 105, 110, 115, 101, 114, 116, 32, 240, 159, 152, 134, 32, 119, 104, 101, 110, 32, 121, 111, 117, 32, 116, 121, 112, 101, 32, 88, 68, 41, 46, 32, 83, 101, 101, 32, 82, 101, 112, 108, 97, 99, 101, 32, 116, 101, 120, 116, 32, 97, 110, 100, 32, 112, 117, 110, 99, 116, 117, 97, 116, 105, 111, 110, 32, 105, 110, 32, 100, 111, 99, 117, 109, 101, 110, 116, 115, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the difference between length of tokens and length of text is because some characters more than one byte."
      ],
      "metadata": {
        "id": "VQn4EIx2Crk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dictionary to calculate pair wise occurences of characters\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids,ids[1:]):\n",
        "        counts[pair] = counts.get(pair,0)+1\n",
        "    return counts"
      ],
      "metadata": {
        "id": "AP0vEKRKCkB7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = get_stats(tokens)"
      ],
      "metadata": {
        "id": "T-pHC4Z8FgXL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(((v,k) for (k,v) in counts.items()), reverse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcD0Mv0BHY-H",
        "outputId": "05f8b53f-1fbd-4dcc-878e-01f637d59b8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(21, (101, 32)), (20, (32, 116)), (13, (116, 104)), (12, (104, 101)), (12, (101, 114)), (11, (114, 32)), (11, (110, 32)), (10, (116, 101)), (10, (32, 97)), (9, (116, 105)), (8, (111, 110)), (8, (32, 101)), (7, (116, 32)), (7, (115, 32)), (7, (114, 101)), (7, (111, 114)), (7, (101, 115)), (6, (117, 32)), (6, (115, 101)), (6, (111, 117)), (6, (110, 115)), (6, (109, 111)), (6, (105, 111)), (6, (101, 120)), (6, (97, 110)), (6, (32, 115)), (5, (121, 32)), (5, (120, 116)), (5, (117, 103)), (5, (116, 111)), (5, (115, 117)), (5, (115, 116)), (5, (111, 106)), (5, (106, 105)), (5, (105, 110)), (5, (103, 103)), (5, (103, 101)), (5, (101, 110)), (5, (101, 109)), (5, (100, 32)), (5, (97, 99)), (5, (32, 111)), (5, (32, 99)), (4, (226, 128)), (4, (121, 111)), (4, (114, 116)), (4, (112, 108)), (4, (112, 101)), (4, (108, 121)), (4, (105, 99)), (4, (105, 32)), (4, (99, 101)), (4, (99, 97)), (4, (97, 114)), (4, (44, 32)), (4, (32, 121)), (4, (32, 119)), (4, (32, 105)), (3, (121, 112)), (3, (117, 116)), (3, (116, 121)), (3, (114, 97)), (3, (111, 109)), (3, (111, 32)), (3, (110, 116)), (3, (108, 97)), (3, (105, 116)), (3, (101, 112)), (3, (101, 100)), (3, (99, 116)), (3, (97, 116)), (3, (97, 108)), (3, (32, 112)), (2, (128, 157)), (2, (128, 156)), (2, (119, 105)), (2, (119, 101)), (2, (117, 109)), (2, (116, 117)), (2, (115, 115)), (2, (115, 46)), (2, (110, 111)), (2, (110, 100)), (2, (109, 101)), (2, (109, 97)), (2, (108, 108)), (2, (108, 105)), (2, (107, 101)), (2, (105, 101)), (2, (104, 97)), (2, (104, 32)), (2, (102, 111)), (2, (101, 119)), (2, (101, 101)), (2, (99, 107)), (2, (97, 117)), (2, (97, 115)), (2, (89, 111)), (2, (86, 105)), (2, (82, 101)), (2, (67, 104)), (2, (45, 69)), (2, (32, 226)), (2, (32, 117)), (2, (32, 114)), (2, (32, 86)), (2, (32, 82)), (2, (32, 67)), (1, (240, 159)), (1, (159, 152)), (1, (157, 44)), (1, (157, 32)), (1, (156, 116)), (1, (156, 104)), (1, (152, 134)), (1, (134, 32)), (1, (121, 45)), (1, (120, 97)), (1, (119, 111)), (1, (119, 104)), (1, (119, 32)), (1, (118, 101)), (1, (117, 115)), (1, (117, 114)), (1, (117, 112)), (1, (117, 110)), (1, (117, 105)), (1, (117, 97)), (1, (116, 226)), (1, (116, 116)), (1, (116, 115)), (1, (116, 97)), (1, (114, 121)), (1, (114, 114)), (1, (114, 111)), (1, (114, 110)), (1, (114, 100)), (1, (114, 46)), (1, (113, 117)), (1, (112, 226)), (1, (112, 117)), (1, (112, 114)), (1, (112, 104)), (1, (112, 58)), (1, (111, 119)), (1, (111, 116)), (1, (111, 115)), (1, (111, 112)), (1, (111, 111)), (1, (111, 99)), (1, (111, 98)), (1, (110, 108)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (110, 45)), (1, (109, 112)), (1, (109, 109)), (1, (109, 98)), (1, (108, 111)), (1, (108, 101)), (1, (108, 32)), (1, (107, 108)), (1, (107, 32)), (1, (105, 112)), (1, (105, 109)), (1, (105, 107)), (1, (105, 44)), (1, (104, 117)), (1, (104, 114)), (1, (104, 111)), (1, (102, 32)), (1, (101, 121)), (1, (101, 118)), (1, (101, 116)), (1, (101, 97)), (1, (101, 46)), (1, (101, 44)), (1, (100, 111)), (1, (100, 105)), (1, (100, 100)), (1, (100, 44)), (1, (99, 117)), (1, (99, 111)), (1, (99, 104)), (1, (98, 117)), (1, (98, 115)), (1, (98, 101)), (1, (97, 109)), (1, (97, 105)), (1, (97, 100)), (1, (97, 32)), (1, (88, 68)), (1, (84, 105)), (1, (83, 101)), (1, (80, 114)), (1, (73, 102)), (1, (71, 108)), (1, (70, 110)), (1, (69, 110)), (1, (69, 46)), (1, (69, 32)), (1, (68, 41)), (1, (67, 108)), (1, (65, 114)), (1, (58, 32)), (1, (46, 84)), (1, (46, 80)), (1, (46, 73)), (1, (46, 69)), (1, (46, 67)), (1, (46, 32)), (1, (41, 46)), (1, (40, 102)), (1, (32, 240)), (1, (32, 113)), (1, (32, 110)), (1, (32, 108)), (1, (32, 107)), (1, (32, 102)), (1, (32, 100)), (1, (32, 98)), (1, (32, 89)), (1, (32, 88)), (1, (32, 83)), (1, (32, 71)), (1, (32, 70)), (1, (32, 65)), (1, (32, 40)), (1, (32, 32))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(101,32) is the pair which has the highest number of occurences.\n"
      ],
      "metadata": {
        "id": "3WNjyODPHnsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chr(101), chr(32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkWOmL3pHfI4",
        "outputId": "ebd6242f-5296-43c0-cab2-0196a52b53b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', ' ')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e and space are the most commonly occuring pair of tokens"
      ],
      "metadata": {
        "id": "gJlmIOufH0eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_pair = max(counts, key=counts.get)"
      ],
      "metadata": {
        "id": "7xXPSb-ZHv5n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYn_Fjd-IHy9",
        "outputId": "1f243f2c-81d8-46ef-bb90-307a77b600dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the number of occurences, we can start to merge the pair and produce a new token number - so vocab size increase and the sequence length decreases - there is usually a sweet spot which is a hyperparameter and is tunable"
      ],
      "metadata": {
        "id": "l5HP5KREJf9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(ids, pair, idx):\n",
        "  new_ids = [] # new list of indices\n",
        "  i = 0\n",
        "  while(i < len(ids)):\n",
        "    if (i < len(ids) - 1) and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)\n",
        "      i+=2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i+=1\n",
        "  return new_ids\n",
        "\n",
        "new_ids = merge([5,6,6,7,9,1], [6,7], 99)\n",
        "print(new_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSciZ27hIP0P",
        "outputId": "5c0e4d57-4555-400b-ea38-361dd15f02d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 6, 99, 9, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: merge our tokens with top_pair as the second argument and replace it with 256\n",
        "\n",
        "new_tokens = merge(tokens, top_pair, 256)\n",
        "print(new_tokens)\n",
        "print(\"length of new tokens is : \",len(new_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFGL7mjHLWs_",
        "outputId": "a0927661-b69b-4305-dc78-58fe2c72c985"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[89, 111, 117, 32, 99, 97, 110, 32, 113, 117, 105, 99, 107, 108, 121, 32, 114, 101, 112, 108, 97, 99, 256, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 101, 109, 111, 106, 105, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 115, 32, 97, 115, 32, 121, 111, 117, 32, 116, 121, 112, 101, 46, 69, 110, 116, 101, 114, 32, 97, 32, 99, 111, 109, 109, 111, 110, 108, 121, 32, 117, 115, 101, 100, 32, 119, 111, 114, 100, 32, 111, 114, 32, 112, 104, 114, 97, 115, 256, 108, 105, 107, 256, 226, 128, 156, 104, 101, 97, 114, 116, 226, 128, 157, 32, 111, 114, 32, 226, 128, 156, 116, 104, 117, 109, 98, 115, 32, 117, 112, 226, 128, 157, 44, 32, 116, 104, 101, 110, 32, 112, 114, 101, 115, 115, 32, 70, 110, 45, 69, 32, 111, 114, 32, 116, 104, 256, 71, 108, 111, 98, 256, 107, 101, 121, 45, 69, 46, 73, 102, 32, 116, 104, 101, 114, 256, 97, 114, 256, 110, 111, 32, 101, 109, 111, 106, 105, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 115, 32, 102, 111, 114, 32, 116, 104, 256, 116, 101, 120, 116, 32, 121, 111, 117, 32, 101, 110, 116, 101, 114, 101, 100, 44, 32, 116, 104, 256, 67, 104, 97, 114, 97, 99, 116, 101, 114, 32, 86, 105, 101, 119, 101, 114, 32, 111, 112, 101, 110, 115, 46, 80, 114, 101, 115, 115, 32, 82, 101, 116, 117, 114, 110, 32, 116, 111, 32, 114, 101, 112, 108, 97, 99, 256, 116, 104, 256, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 116, 104, 256, 115, 117, 103, 103, 101, 115, 116, 101, 100, 32, 101, 109, 111, 106, 105, 44, 32, 111, 114, 32, 99, 104, 111, 111, 115, 256, 97, 110, 111, 116, 104, 101, 114, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 46, 67, 108, 105, 99, 107, 32, 116, 104, 256, 65, 114, 114, 111, 119, 32, 98, 117, 116, 116, 111, 110, 32, 32, 116, 111, 32, 115, 101, 256, 97, 100, 100, 105, 116, 105, 111, 110, 97, 108, 32, 115, 117, 103, 103, 101, 115, 116, 105, 111, 110, 115, 32, 97, 110, 100, 32, 101, 109, 111, 106, 105, 32, 105, 110, 32, 116, 104, 256, 67, 104, 97, 114, 97, 99, 116, 101, 114, 32, 86, 105, 101, 119, 101, 114, 46, 84, 105, 112, 58, 32, 89, 111, 117, 32, 99, 97, 110, 32, 97, 117, 116, 111, 109, 97, 116, 105, 99, 97, 108, 108, 121, 32, 105, 110, 115, 101, 114, 116, 32, 97, 110, 32, 101, 109, 111, 106, 105, 32, 101, 118, 101, 114, 121, 32, 116, 105, 109, 256, 121, 111, 117, 32, 116, 121, 112, 256, 99, 101, 114, 116, 97, 105, 110, 32, 116, 101, 120, 116, 32, 40, 102, 111, 114, 32, 101, 120, 97, 109, 112, 108, 101, 44, 32, 97, 117, 116, 111, 109, 97, 116, 105, 99, 97, 108, 108, 121, 32, 105, 110, 115, 101, 114, 116, 32, 240, 159, 152, 134, 32, 119, 104, 101, 110, 32, 121, 111, 117, 32, 116, 121, 112, 256, 88, 68, 41, 46, 32, 83, 101, 256, 82, 101, 112, 108, 97, 99, 256, 116, 101, 120, 116, 32, 97, 110, 100, 32, 112, 117, 110, 99, 116, 117, 97, 116, 105, 111, 110, 32, 105, 110, 32, 100, 111, 99, 117, 109, 101, 110, 116, 115, 46]\n",
            "length of new tokens is :  596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids,ids[1:]):\n",
        "        counts[pair] = counts.get(pair,0)+1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  new_ids = [] # new list of indices\n",
        "  i = 0\n",
        "  while(i < len(ids)):\n",
        "    if (i < len(ids) - 1) and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)\n",
        "      i+=2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i+=1\n",
        "  return new_ids\n",
        "\n",
        "vocab_size = 276 # desired vocab_size\n",
        "num_merges = vocab_size-256\n",
        "ids = list(tokens) # copy of tokens\n",
        "merges = {}\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats,key=stats.get)\n",
        "  idx = 256+i\n",
        "  ids = merge(ids,pair,idx)\n",
        "  print(f'merging {pair} into {idx}')\n",
        "  merges[pair] = idx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9jhnmdiPljj",
        "outputId": "5a9a7f4d-438e-40b8-d312-f8ece2f6f296"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) into 256\n",
            "merging (32, 116) into 257\n",
            "merging (101, 114) into 258\n",
            "merging (110, 32) into 259\n",
            "merging (116, 105) into 260\n",
            "merging (257, 104) into 261\n",
            "merging (116, 32) into 262\n",
            "merging (32, 101) into 263\n",
            "merging (101, 115) into 264\n",
            "merging (115, 32) into 265\n",
            "merging (111, 114) into 266\n",
            "merging (111, 117) into 267\n",
            "merging (109, 111) into 268\n",
            "merging (260, 111) into 269\n",
            "merging (261, 256) into 270\n",
            "merging (97, 99) into 271\n",
            "merging (116, 101) into 272\n",
            "merging (120, 262) into 273\n",
            "merging (116, 104) into 274\n",
            "merging (268, 106) into 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original list length: \", len(tokens))\n",
        "print(\"Merged list length : \", len(ids))\n",
        "print(\"Compression ratio : \", len(tokens)/ len(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Q3eTOMTjxs",
        "outputId": "6133ffcf-6510-4a53-c02d-6ce095f6f815"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original list length:  617\n",
            "Merged list length :  461\n",
            "Compression ratio :  1.3383947939262473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let us do the same for a bigger text."
      ],
      "metadata": {
        "id": "RGuM2H1AUFNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_text = ''' I‚Äôm assuming, if you are on the Internet and reading kind of a nerdy blog, that you know what Unicode is. At the very least, you have a very general understanding of it ‚Äî maybe ‚Äúit‚Äôs what gives us emoji‚Äù.\n",
        "\n",
        "That‚Äôs about as far as most people‚Äôs understanding extends, in my experience, even among programmers. And that‚Äôs a tragedy, because Unicode has a lot of‚Ä¶ ah, depth to it. Not to say that Unicode is a terrible disaster ‚Äî more that human language is a terrible disaster, and anything with the lofty goals of representing all of it is going to have some wrinkles.\n",
        "\n",
        "So here is a collection of curiosities I‚Äôve encountered in dealing with Unicode that you generally only find out about through experience. Enjoy.\n",
        "\n",
        "Also, I strongly recommend you install the Symbola font, which contains basic glyphs for a vast number of characters. They may not be pretty, but they‚Äôre better than seeing the infamous Unicode lego.\n",
        "\n",
        "Some definitions\n",
        "\n",
        "There are already plenty of introductions to Unicode floating around (wikipedia, nedbat, joel), and this is not going to be one. But here‚Äôs a quick refresher.\n",
        "\n",
        "Unicode is a big table that assigns numbers (codepoints) to a wide variety of characters you might want to use to write text. We often say ‚ÄúUnicode‚Äù when we mean ‚Äúnot ASCII‚Äù, but that‚Äôs silly since of course all of ASCII is also included in Unicode.\n",
        "\n",
        "UTF-8 is an encoding, a way of turning a sequence of codepoints into bytes. All Unicode codepoints can be encoded in UTF-8. ASCII is also an encoding, but only supports 128 characters, mostly English letters and punctuation.\n",
        "\n",
        "A character is a fairly fuzzy concept. Letters and numbers and punctuation are characters. But so are Braille and frogs and halves of flags. Basically a thing in the Unicode table somewhere.\n",
        "\n",
        "A glyph is a visual representation of some symbol, provided by a font. It might represent a single character, or it might represent several. Or both!\n",
        "\n",
        "Unicode is divided into seventeen planes, numbered zero through sixteen. Plane 0 is also called the Basic Multilingual Plane, or just BMP, so called because it contains the alphabets of most modern languages. The other planes are much less common and are sometimes informally referred to as the astral planes.\n",
        "\n",
        "Everything you know about text is wrong\n",
        "\n",
        "If the only written languge you‚Äôre familiar with is English, that goes doubly so.\n",
        "\n",
        "Perhaps you want to sort text. A common enough problem. Let‚Äôs give this a try in Python. To simplify things, we‚Äôll even stick to English text.\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        ">>> words = ['cafeteria', 'caffeine', 'caf√©']\n",
        ">>> words.sort()\n",
        ">>> words\n",
        "['cafeteria', 'caffeine', 'caf√©']\n",
        "Oops. Turns out Python‚Äôs sorting just compares by Unicode codepoint, so the English letter ‚Äú√©‚Äù (U+00E9) is greater than the English letter ‚Äúf‚Äù (U+0066).\n",
        "\n",
        "Did you know the German letter ‚Äú√ü‚Äù is supposed to sort equivalently to ‚Äúss‚Äù? Where do you sort the Icelandic letter ‚Äú√¶‚Äù? What about the English ligature ‚Äú√¶‚Äù, which is the same character?\n",
        "\n",
        "What about case? The Turkish dotless ‚Äúƒ±‚Äù capitalizes to the familiar capital ‚ÄúI‚Äù, but in Turkish, the lowercase of that is ‚Äúƒ±‚Äù and the uppercase of ‚Äúi‚Äù is ‚Äúƒ∞‚Äù. Is uppercase ‚Äú√ü‚Äù the more traditional ‚ÄúSS‚Äù, or maybe ‚ÄúSs‚Äù, or the somewhat recent addition ‚Äú·∫û‚Äù?\n",
        "\n",
        "Or, how do you compare equality? Is ‚Äú√ü‚Äù equal to ‚Äúss‚Äù? Is ‚Äú√¶‚Äù equal to ‚Äúae‚Äù? Is ‚Äú√©‚Äù equal to ‚Äú√©‚Äù?\n",
        "\n",
        "Ah, you say! I‚Äôve heard about this problem and know how to solve it. I can just throw Unicode normalization at it, which will take care of combining characters and all that other nonsense. I can even strip out all combining characters and have nice normal English text left, because for some reason I am under the impression that English text is ‚Äúnormal‚Äù and all else is ‚Äúabnormal‚Äù.\n",
        "\n",
        "Sure, let‚Äôs give that a try.\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        ">>> import unicodedata\n",
        ">>> normalize = lambda s: ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))\n",
        ">>>\n",
        ">>> normalize('Pok√©mon')\n",
        "'Pokemon'\n",
        "Great, problem solved.\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        ">>> normalize('ƒ±')\n",
        "'ƒ±'\n",
        ">>> normalize('√¶')\n",
        "'√¶'\n",
        ">>> normalize('√ü')\n",
        "'√ü'\n",
        "Hmm.\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        ">>> normalize('ÌïúÍ∏Ä')\n",
        "'ÌïúÍ∏Ä'\n",
        ">>> normalize('„Ç§„Éº„Éñ„Ç§')\n",
        "'„Ç§„Éº„Éï„Ç§'\n",
        "Uh oh.\n",
        "\n",
        "Yes, it turns out that Unicode decomposition also decomposes Hangul (the alphabet used to write Korean) into its sub-components, which then may or may not still even render correctly, as well as splitting the diacritics off of Japanese kana, which significantly alters the pronunciation and meaning. Almost as if Unicode decomposition was never meant to help programmers forcibly cram the entire world back into ASCII.\n",
        "\n",
        "Even if you only care about English text, there‚Äôs more than one Latin alphabet in Unicode! Is ‚Äúx‚Äù equivalent to ‚Äúùóë‚Äù or ‚ÄúùòÖ‚Äù or ‚Äúùòπ‚Äù or ‚Äúùô≠‚Äù or ‚Äúùö°‚Äù or ‚ÄúÔΩò‚Äù or ‚Äúùê±‚Äù? What about ‚Äú√ó‚Äù or ‚Äú—Ö‚Äù or ‚Äú‚®Ø‚Äù or ‚Äú‚Öπ‚Äù? Ah, sorry, those last four are actually the multiplication sign, a Cyrillic letter, the symbol for cross product, and the Roman numberal for ten.\n",
        "\n",
        "This is a particularly aggravating problem because most programming languages have facilities for comparing and changing the case of text built in, and most of them are extremely na√Øve about it. You can‚Äôt even correctly change the case of English-looking text without knowing what locale it came from ‚Äî the title-case of ‚Äúistanbul‚Äù may actually be ‚Äúƒ∞stanbul‚Äù depending on language, because of Turkish‚Äôs dotted ‚Äúi‚Äù.\n",
        "\n",
        "The only library I‚Äôm aware of off the top of my head for correctly dealing with any of these problems is ICU, which is a hulking monstrosity hardly suited for shipping as part of a programming language. And while their homepage does list a lot of impressive users, I‚Äôve only encountered it in code I‚Äôve worked on once.\n",
        "\n",
        "Combining characters and character width\n",
        "\n",
        "Typically we think of combining characters as being the floating diacritical marks that can latch onto the preceding letter, such as using U+0301 COMBINING ACUTE ACCENT to make ‚ÄúqÃÅ‚Äù, in case we are direly in need of it for some reason. There are a few other combining ‚Äúdiacriticals‚Äù that aren‚Äôt so related to language; for example, U+20E0 COMBINING ENCLOSING CIRCLE BACKSLASH can produce ‚Äú√©‚É†‚Äù, the universal symbol for ‚Äúmy software only supports English, and also I am not aware that English has diacritics too‚Äù. Or perhaps you‚Äôd use U+20E3 COMBINING ENCLOSING KEYCAP to make ‚Äú√©‚É£‚Äù and indicate that the user should press their √© key.\n",
        "\n",
        "All of these have an impact on the ‚Äúlength‚Äù of a string. You could write either of those ‚Äú√©‚Äù sequences with three codepoints: the letter ‚Äúe‚Äù, the combining accent, and the combining border. But clearly they each only contribute one symbol to the final text. This isn‚Äôt a particularly difficult problem; just ignore combining characters when counting, right?\n",
        "\n",
        "More interesting are the Unicode characters that are not combining characters, but compose in some way in practice anyway. The flag emoji, for example, don‚Äôt actually exist in Unicode. The Unicode Consortium didn‚Äôt want to be constantly amending a list of national flags as countries popped in and out of existence, so instead they cheated. They added a set of 26 regional indicator symbols, one for each letter of the English alphabet, and to encode a country‚Äôs flag you write its two-letter ISO country code with those symbols. So the Canadian flag, üá®üá¶, is actually the two characters U+1F1E8 REGIONAL INDICATOR SYMBOL LETTER C and U+1F1E6 REGIONAL INDICATOR SYMBOL LETTER A. But if you put a bogus combination together, you probably won‚Äôt get a flag glyph; you‚Äôll get stand-ins for the characters instead. (For example, üáøüáø.) So the ‚Äúlength‚Äù of a pair of these characters depends both on the display font (which may not support all flags), and on the current geopolitical state of the world. How‚Äôs that for depending on global mutable state?\n",
        "\n",
        "But it gets better! There‚Äôs a character called U+200D ZERO WIDTH JOINER, which is used to combine otherwise distinct characters in some languages (but has fairly general semantics). Apple has made creative use of this character to compose emoji together. The report on emoji has some examples. So now the length of some text is completely arbitrary, based on whatever arbitrary ligatures the font includes.\n",
        "\n",
        "To be fair, that was already true anyway. You might argue that the length of text in human terms is not actually all that interesting a quantity, and you‚Äôd be right, but that‚Äôs why this section is about character width. Because I‚Äôm typing in a terminal right now, and terminals fit all their text in a grid.\n",
        "\n",
        "Let‚Äôs return to the simpler world of letters and revisit that Hangul example:\n",
        "\n",
        "1\n",
        "2\n",
        ">>> normalize('ÌïúÍ∏Ä')\n",
        "'ÌïúÍ∏Ä'\n",
        "Hangul characters are actually blocks composed of exactly three parts called Jamo. (Here‚Äôs gritty detail on Hangul, Jamo, and Unicode. It‚Äôs a really cool alphabet.) Applying Unicode decomposition actually breaks each character down into its component Jamo, which are then supposed to render exactly the same as the original. They aren‚Äôt marked as combining characters in the Unicode database, but if you have three of them in a row (arranged sensibly), you should only see one character. The actual decomposition for the text above is ‚Äú„Öé„Öè„Ñ¥ „Ñ±„Ö°„Ñπ‚Äù, written with separate characters that don‚Äôt combine. There are a good few languages that work this way ‚Äî Devanagari (the script used for Hindi et al.) and Bengali rely heavily on character composition, and Hebrew uses it for rendering vowels.\n",
        "\n",
        "And yet I ended up with four very different renderings. In this blog post, with my default monospace font, I see the full sequence of six Jamo. If I paste the same text somewhere with a proportional font, I see something very nearly identical to the original characters, albeit slightly fuzzier from being generated on the fly. In Konsole, I see only the first Jamo for each character: '„Öé„Ñ±'. (This has been fixed as of July 6, 2016, though I don‚Äôt know what Konsole release contains the fix.) And in my usual libvte-based terminal, the combining behavior falls apart, and I see a nonsensical mess that I can‚Äôt even reproduce with Unicode:\n",
        "\n",
        "Screenshot of mangled Hangul in a terminal; several characters overlap\n",
        "\n",
        "I can only guess at what happened here. Clearly both terminals decided that each set of three Jamo was only one character wide, but for some reason they didn‚Äôt combine. Konsole adamantly refuses to render any Jamo beyond the first, even if I enter them independently; VTE dutifully renders them all but tries to constrain them to the grid, leading to overlap.\n",
        "\n",
        "This is not the first width-related problem I‚Äôve encountered with Unicode and terminals. Consider emoji, which tend to be square in shape. I might reasonably want to say to someone on IRC: ‚Äúhappy birthday! üéÅ hope it‚Äôs a good one.‚Äù (That‚Äôs U+1F380 WRAPPED PRESENT, if you didn‚Äôt take my advice and install Symbola.) But I use a terminal IRC client, and here‚Äôs how that displays, in VTE and Konsole:\n",
        "\n",
        "Screenshot of the sentence in VTE; the birthday gift overlaps the following space\n",
        "\n",
        "Screenshot of the sentence in Konsole; the spacing is correct but the cursor position is wrong\n",
        "\n",
        "You can see how VTE has done the same thing as with Hangul: it thinks the emoji should only take up one character cell, but dutifully renders the entire thing, allowing the contents to spill out and overlap the following space. You might think Konsole has gotten this one right, but look carefully ‚Äî the final quote is slightly overlapping the cursor. Turns out that Konsole will print each line of text as regular text, so any character that doesn‚Äôt fit the terminal grid will misalign every single character after it. The cursor (and selection) is always fit to the grid, so if you have several emoji in the same row, the cursor might appear to be many characters away from its correct position. There are several bugs open on Konsole about this, dating back many years, with no response from developers. I actually had to stop using Konsole because of this sole issue, because I use ‚öò U+2698 FLOWER as my shell prompt, which misaligned the cursor every time.\n",
        "\n",
        "All of these problems can be traced back to the same source: a POSIX function called wcwidth, which is intended to return the number of terminal columns needed to display a given character. It exists in glibc, which sent me on a bit of a wild goose chase. I originally thought that wcwidth must be reporting that the second and third Jamo characters are zero width, but this proved not to be the case:\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        ">>> libc.wcwidth(c_wchar('\\u1100'))  # initial Jamo\n",
        "2\n",
        ">>> libc.wcwidth(c_wchar('\\u1161'))  # second Jamo\n",
        "1\n",
        "Well, it seems Konsole actually implements its own wcwidth which appears to be based on this original implementation. Both versions preserve an innocuous comment that explains quite a lot:\n",
        "\n",
        "Hangul Jamo medial vowels and final consonants (U+1160-U+11FF) have a column width of 0.\n",
        "\n",
        "Aha. So Konsole saw that the second and third Jamo took zero space, so it didn‚Äôt bother trying to print them at all.\n",
        "\n",
        "Then what the hell is VTE doing? It defers to some utility functions in glib (GNOME‚Äôs library of‚Ä¶ stuff), such as g_unichar_iszerowidth, which‚Ä¶ explicitly says yes for everything between U+1160 and U+1200. Wouldn‚Äôt you know it, those are the secondary and tertiary Jamo characters. So VTE saw that they took zero space, so it didn‚Äôt make any extra room for them, but still tried to print them. I expect they didn‚Äôt combine in VTE because VTE has no idea they‚Äôre supposed to combine, so it printed each one individually.\n",
        "\n",
        "Oh, but this madness gets even better. WeeChat, another terminal IRC client, outright strips emoji, everywhere. This is apparently the fault of‚Ä¶ glibc‚Äôs implementation of wcwidth, which defaults to 1 for printable characters and 0 otherwise, which requires knowing what the characters are, which oops doesn‚Äôt work so well when glibc was using a vendored copy of the (pre-emoji) Unicode 5.0 database until glibc 2.22, which was released less than a month ago.\n",
        "\n",
        "Beloved SSH replacement mosh has a similar problem, in this case blamed on the wcwidth implementation shipped with OS X. Gosh, I thought Apple was on the ball with Unicode.\n",
        "\n",
        "We‚Äôre now up to at least four mutually incompatible and differently broken versions of this same function. Lovely.\n",
        "\n",
        "I might be on the fringe here, but I‚Äôm pretty adamant that having a communication program silently and invisibly eat parts of your text is a bad thing.\n",
        "\n",
        "While I‚Äôm at it: why are emoji left with a width of 1? They tend to be drawn to fit a square, just like CJK characters (which are why we need double-width character support in the first place), and they‚Äôre even of Japanese origin. My rendering problems would go away in both terminals if they used widths of 2. Hell, I‚Äôm going to go file bugs on both of them right now.\n",
        "\n",
        "You will not go to space today\n",
        "\n",
        "Sometimes you care about whitespace. Perhaps you‚Äôre using it to separate words. In, say, a programming language. Like JavaScript.\n",
        "\n",
        "1\n",
        "alert(2+·öÄ40);\n",
        "JavaScript‚Äôs syntax defers the decision of what counts as whitespace to the Unicode database, which assigns a WSpace property to a handful of codepoints. Seems like a good approach, except for this one unusual exception: ‚Äù·öÄ‚Äù is a space character, U+1680 OGHAM SPACE MARK. Ogham is an alphabet used in older forms of Irish, and its space character generally renders as a line. Surprise!\n",
        "\n",
        "Complicating this somewhat further, there are actually two definitions of whitespace in Unicode. Unicode assigns every codepoint a category, and has three categories for what sounds like whitespace: ‚ÄúSeparator, space‚Äù; ‚ÄúSeparator, line‚Äù; and ‚ÄúSeparator, paragraph‚Äù.\n",
        "\n",
        "If you‚Äôre familiar with Unicode categories, you might be tempted to use these to determine what characters are whitespace. Except that CR, LF, tab, and even vertical tab are all categorized as ‚ÄúOther, control‚Äù and not as separators. You might think that at least LF should count as a line separator, but no; the only character in the ‚ÄúSeparator, line‚Äù category is U+2028 LINE SEPARATOR, and the only character in ‚ÄúSeparator, paragraph‚Äù is U+2029 PARAGRAPH SEPARATOR. I have never seen either of them used, ever. Thankfully, all of these have the WSpace property.\n",
        "\n",
        "As an added wrinkle, the lone oddball character ‚Äú‚†Ä‚Äù renders like a space in most fonts. But it‚Äôs not whitespace, it‚Äôs not categorized as a separator, and it doesn‚Äôt have WSpace. It‚Äôs actually U+2800 BRAILLE PATTERN BLANK, the Braille character with none of the dots raised. (I say ‚Äúmost fonts‚Äù because I‚Äôve occasionally seen it rendered as a 2√ó4 grid of open circles.) Everything is a lie.\n",
        "\n",
        "JavaScript has no string type\n",
        "\n",
        "JavaScript‚Äôs ‚ÄúString‚Äù type (or ‚Äústring‚Äù type?) is not actually a string type. Observe:\n",
        "\n",
        "1\n",
        "2\n",
        "var bomb = \"üí£\";\n",
        "console.log(bomb.length);  // 2\n",
        "That‚Äôs a string containing a single character, U+1F4A3 BOMB. Yet JavaScript thinks it contains two! What on earth is going on here? Let‚Äôs see what JavaScript thinks those two characters are, using charCodeAt.\n",
        "\n",
        "1\n",
        "2\n",
        "console.log(bomb.charCodeAt(0).toString(16));  // d83d\n",
        "console.log(bomb.charCodeAt(1).toString(16));  // dca3\n",
        "These aren‚Äôt actually characters. Everything from U+D800 through U+DFFF is permanently reserved as a non-character for the sake of encoding astral plane characters in UTF-16. The short version is that all BMP characters are two bytes in UTF-16, and all astral plane characters are two of these non-characters (called a surrogate pair) for a total of four bytes.\n",
        "\n",
        "JavaScript‚Äôs string type is backed by a sequence of unsigned 16-bit integers, so it can‚Äôt hold any codepoint higher than U+FFFF and instead splits them into surrogate pairs. I argue that a string isn‚Äôt a string if it can‚Äôt hold a sequence of arbitrary characters, and JavaScript strings can‚Äôt directly contain astral plane characters, so they don‚Äôt qualify.\n",
        "\n",
        "I rag on JavaScript, but this is an old problem. C strings (well, char*) are just sequences of bytes, so you can‚Äôt fit more than Latin-1. Some libraries have historically tried to address this with ‚Äúwide strings‚Äù, wchar_t*, but the size of wchar_t is implementation-defined and 16 bits on Windows, where the entire OS API has the same problem as JavaScript.\n",
        "\n",
        "Arguably, 16-bit faux strings are worse than 8-bit faux strings. It becomes pretty obvious pretty quickly that 8 bits is not enough to fit more than some European alphabets, and anyone but the most sheltered programmer is forced to deal with it the first time they encounter an em dash. But 16 bits covers the entire BMP, which contains all current languages, some ancient languages, dingbats, mathematical symbols, and tons of punctuation. So if you have 16-bit faux strings, it‚Äôs very easy to think you have all of Unicode automatically handled and then be sorely mistaken. Thankfully, the increasing availability and popularity of emoji, which are mostly not in the BMP (but see below), makes astral plane support a more practical matter.\n",
        "\n",
        "This probably all dates back to the original design of Unicode, which assumed that we‚Äôd never possibly need any more than 65,536 different characters and promised that two bytes would be enough for everyone. Oops.\n",
        "\n",
        "(This is the same reason that Chinese hanzi and Japanese kanji are merged into a single set of codepoints: they‚Äôre both huge alphabets and it was the only way to fit them both into two bytes. This is called Han unification, and I have seen it end friendships, so I prefer not to discuss it further.)\n",
        "\n",
        "One more trivium: MySQL has a utf8 encoding, and it‚Äôs generally regarded as best practice to use that for all your text columns so you can store Unicode. But, oops, MySQL arbitrarily limits it to three bytes per character, which isn‚Äôt enough to encode most astral plane characters! What a great technical decision and not at all yet another thorn in the unusable sinkhole that is MySQL. Version 5.5 introduced a utf8mb4 encoding that fixes this, so have fun ALTERing some multi-gigabyte tables in production.\n",
        "\n",
        "There's no such thing as emoji\n",
        "\n",
        "I exaggerate slightly.\n",
        "\n",
        "The word ‚Äúemoji‚Äù is generally used to mean ‚Äúany character that shows as a colored picture on my screen‚Äù, much like the word ‚ÄúUnicode‚Äù is generally used to mean ‚Äúany character not on my US QWERTY keyboard‚Äù. So what characters qualify as emoji?\n",
        "\n",
        "There‚Äôs actually no Unicode block called ‚Äúemoji‚Äù. The set of smiley faces is in a block called Emoticons, and most of the rest are in Miscellaneous Symbols and Pictographs and Transport and Map Symbols.\n",
        "\n",
        "The Unicode Consortium has a technical report about emoji, which should be an immediate hint that this is not a trivial matter. In fact the report defines two levels of emoji, and look at how arbitrary these definitions are:\n",
        "\n",
        "emoji character ‚Äî A character that is recommended for use as emoji.\n",
        "\n",
        "level 1 emoji character ‚Äî An emoji character that is among those most commonly supported as emoji by vendors at present.\n",
        "\n",
        "level 2 emoji character ‚Äî An emoji character that is not a level 1 emoji character.\n",
        "\n",
        "So emoji are defined somewhat arbitrarily, and even based on what‚Äôs treated as an emoji in the wild.\n",
        "\n",
        "It‚Äôs tempting to just say that those few astral plane blocks are emoji, but you might be surprised at what else qualifies sometimes. There‚Äôs also a data table listing emoji levels, and it classifies as emoji a good handful of arrows and dingbats and punctuation, even though they‚Äôve been in Unicode for many years. üÉè U+1F0CF PLAYING CARD BLACK JOKER is a level 1 emoji, but nothing else in the entire Playing Cards block qualifies. Similarly, üÄÑ U+1F004 MAHJONG TILE RED DRAGON is the only representative of Mahjong Tiles, and Domino Tiles aren‚Äôt represented at all.\n",
        "\n",
        "I stress, also, that a colored graphic is not the only way emoji (however you define them) may be rendered. Here‚Äôs a screenshot of part of that table on my desktop:\n",
        "\n",
        "Screenshot of emoji rendered as simple outlines\n",
        "\n",
        "That font is Symbola, which only has monochrome vector glyphs. So they‚Äôre no different than any other character.\n",
        "\n",
        "I‚Äôve been seeing an increasing trend lately of treating emoji as somehow completely unique. The IM programs WhatsApp and Telegram both use Apple‚Äôs emoji font on every platform, and I‚Äôve seen even technically-inclined people passionately argue that this is a good state of affairs, because it means both parties will see exactly the same pixels. Wouldn‚Äôt want to confuse anyone by having them see a slightly different image of a steak! (You‚Äôd think that‚Äôs what sending images is for, but what do I know.)\n",
        "\n",
        "This is somewhat troubling to me. The entire point of having these symbols exist in Unicode is so they can be transferred between different systems and treated just like any other text, because now they‚Äôre just text. They aren‚Äôt special in any way (besides being in an astral plane, I suppose), and there‚Äôs no reason you couldn‚Äôt construct an emoji font that displayed regular English characters as graphics. Hell, if you‚Äôre using Firefox, here‚Äôs a demo of SVG embedded in an OpenType font that displays the letter ‚Äúo‚Äù as an animated soccer ball. '''"
      ],
      "metadata": {
        "id": "BvdH5pa0UCNq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(bigger_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1971qFtTnkJ",
        "outputId": "6bd69ba5-35d0-4a42-c946-3b4d6302b667"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Read through the code in the above cells and replicate the same for this bigger text. i.e convert the characters in the string to UTF-8 tokens, find the stats using get_stats API, merge them to make the vocab size equal to 276 and print compression ratio\n",
        "\n",
        "# Convert the characters in the bigger_text string to UTF-8 tokens.\n",
        "bigger_tokens = bigger_text.encode('utf-8')\n",
        "bigger_tokens = list(map(int,bigger_tokens))\n",
        "\n",
        "\n",
        "\n",
        "# Merge the stats to make the vocab size equal to 276.\n",
        "vocab_size = 276 # desired vocab_size\n",
        "num_merges = vocab_size-256\n",
        "ids = list(bigger_tokens) # copy of tokens\n",
        "merges = {}\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats,key=stats.get)\n",
        "  idx = 256+i\n",
        "  ids = merge(ids,pair,idx)\n",
        "  print(f'merging {pair} into {idx}')\n",
        "  merges[pair] = idx\n",
        "\n",
        "# Print the compression ratio.\n",
        "compression_ratio = len(bigger_tokens) / len(ids)\n",
        "print(f\"Compression ratio: {compression_ratio}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z97c6pCUUw3m",
        "outputId": "361e06e0-b645-4cb5-849e-68604afb05bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) into 256\n",
            "merging (32, 116) into 257\n",
            "merging (32, 97) into 258\n",
            "merging (105, 110) into 259\n",
            "merging (101, 114) into 260\n",
            "merging (116, 32) into 261\n",
            "merging (115, 32) into 262\n",
            "merging (257, 104) into 263\n",
            "merging (226, 128) into 264\n",
            "merging (111, 110) into 265\n",
            "merging (104, 97) into 266\n",
            "merging (101, 110) into 267\n",
            "merging (121, 32) into 268\n",
            "merging (44, 32) into 269\n",
            "merging (100, 32) into 270\n",
            "merging (111, 114) into 271\n",
            "merging (97, 108) into 272\n",
            "merging (259, 103) into 273\n",
            "merging (97, 99) into 274\n",
            "merging (99, 111) into 275\n",
            "Compression ratio: 1.2574814973720905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(bigger_tokens), len(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gvnl0LGWAA0",
        "outputId": "b27b1db4-d8e0-4fec-ec52-6b92b9f4f58e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23447 18646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the tokenization process is a completely independent one which is not related to LLMs. LLMs is only going to see the tokens as integers and no text is involved. Tokenization has its own training, processes etc."
      ],
      "metadata": {
        "id": "CxG-sV76WNto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization involves two processes : Encoding - conversion from texts to integers and decoding : conversion from integers back to python string"
      ],
      "metadata": {
        "id": "jntB6zE_x8vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {idx : bytes([idx]) for idx in range(256)}\n",
        "for (p0,p1), idx in merges.items():\n",
        "  vocab[idx] = vocab[p0] + vocab[p1] # byte concatenation\n"
      ],
      "metadata": {
        "id": "AYZRafkf2V1I"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding\n",
        "def decode(ids):\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode('utf-8',errors=\"replace\")\n",
        "  return text\n",
        "\n"
      ],
      "metadata": {
        "id": "UQD2OWt1uW2R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode([98])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B0O2DIxZ27uB",
        "outputId": "52133fca-50d5-4656-e968-0161672219a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding string to tokens"
      ],
      "metadata": {
        "id": "mt_gkQ-li-IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  tokens = list(text.encode('utf-8'))\n",
        "  #Merge the tokens [byte pair encoding]\n",
        "  while len(tokens)>=2:\n",
        "    stats= get_stats(tokens)\n",
        "    pair = min(stats, key = lambda p : merges.get(p,float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break # no more merging to do\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens,pair,idx)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "27juluWk29e4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello world!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppEnDf2cmr1n",
        "outputId": "6f6087de-2a97-4b6e-99f2-7e29a09eb1b2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104, 101, 108, 108, 111, 32, 119, 271, 108, 100, 33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnXMHZpQmwQG",
        "outputId": "aed7fba9-8569-467b-9e49-477aa4e966ae"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some test cases"
      ],
      "metadata": {
        "id": "YryLXyw0nfQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g36uBTtBm6yW",
        "outputId": "5622576b-8479-431d-9ddb-1d72eca4bbb1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple tokenizer - large LLMs like GPT, Llama use a complicated tokenizer."
      ],
      "metadata": {
        "id": "tYpj48W5pkM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT tokenizer first splits the entire text into chunks of sub texts, because putting the entire text through encode + decode naively is sub-optimal, it loses some context and semantic understanding.\n",
        "Instead using RegEx, it is split based on some rules and on these chunks of text, encode + decode processes are done to move back and forth between raw strings and integers used in transformers"
      ],
      "metadata": {
        "id": "2tjlYViYsmhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
      ],
      "metadata": {
        "id": "WBVAaJt-nixH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "some common apostrophes, 've are all taken as separate substrings for tokenization"
      ],
      "metadata": {
        "id": "p1eyLkbjvglZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let us experiment with openai tokenizer library - tiktoken"
      ],
      "metadata": {
        "id": "JfL0aLi5xJlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2cNJaM-vGJa",
        "outputId": "34dd63c2-4c46-4eac-c948-a6539793928a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "TeqAQ-5nxWSd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_gpt2 = tiktoken.get_encoding('gpt2')\n",
        "print(enc_gpt2.encode(\"   hello world!!!!\"))\n",
        "enc_gpt4 = tiktoken.get_encoding('cl100k_base')\n",
        "print(enc_gpt4.encode(\"   hello world!!!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGsUjBxUxc2o",
        "outputId": "c674088e-5823-4d44-e7fc-f6e53851dd70"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220, 220, 23748, 995, 13896]\n",
            "[256, 24748, 1917, 12340]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we see that in gpt2 , spaces are unmerged and tokens are allocated for each of the spaces, in gpt4 the spaces are merged."
      ],
      "metadata": {
        "id": "TmYCNORfyOLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in gpt2, the length of the vocab is 50257 tokens\n"
      ],
      "metadata": {
        "id": "Bm2XbDMZEueU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONDM3yaZxt8K",
        "outputId": "d9d84680-5adc-4dcb-da1b-cacf9f7f4be6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-25 11:43:23--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.179.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.179.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: ‚Äòvocab.bpe‚Äô\n",
            "\n",
            "vocab.bpe           100%[===================>] 445.62K  1.47MB/s    in 0.3s    \n",
            "\n",
            "2024-02-25 11:43:23 (1.47 MB/s) - ‚Äòvocab.bpe‚Äô saved [456318/456318]\n",
            "\n",
            "--2024-02-25 11:43:23--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.179.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.179.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‚Äòencoder.json‚Äô\n",
            "\n",
            "encoder.json        100%[===================>]   1018K  2.38MB/s    in 0.4s    \n",
            "\n",
            "2024-02-25 11:43:24 (2.38 MB/s) - ‚Äòencoder.json‚Äô saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,json\n",
        "\n",
        "with open('encoder.json','r') as f:\n",
        "    encoder = json.load(f)\n",
        "\n",
        "with open('vocab.bpe','r', encoding='utf-8') as f:\n",
        "  bpe_data = f.read()\n",
        "\n",
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n"
      ],
      "metadata": {
        "id": "w9uSEShtE7IF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoder) # 50257"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGbMkDI6GFNy",
        "outputId": "bcd12e8e-c378-439b-e685-7feb815579ee"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "256 raw byte tokens and then there are 50000 merges. the last character is EOD [a special token]. kind of like reset after this special character is seen [a delimiter]"
      ],
      "metadata": {
        "id": "KvH9o5jlGLq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder['.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWljjazwGG7R",
        "outputId": "e17fa554-9f4c-4b18-844d-026359c65a87"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "more info on special tokens can be obtained by going throught he tiktoken library written in Rust\n",
        "\n",
        "There are some other special tokens like End of Prompt, FiM_Prefix -> Fill in the middle, fill in the middle - suffix"
      ],
      "metadata": {
        "id": "zGKrSpuxHKa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Implement your own GPT 4 tokenizer based on the guidelines in minbpe repo and taking that as reference. Train the vocab on a large dataset having a lot of diverse character sets."
      ],
      "metadata": {
        "id": "xl3tuZAoItzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another common tokenizer : Sentencepiece - can do both training and inference of tokenization , it is used by LLama and Mistral series.\n"
      ],
      "metadata": {
        "id": "qY1Uc7klI9hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the sentencepiece method applies bpe on the unicode points direclty instead of encoding the text using utf-8 bytes and then applying bpe.\n",
        "It only falls back to byte level when there is a rare character or there is another hyperparamter called the character_coverage which is triggered when a before seen character appears."
      ],
      "metadata": {
        "id": "Wo1mFemEJ8Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "cMYIwEbOGfMt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a toy.txt file with some random text\n",
        "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
      ],
      "metadata": {
        "id": "6YfHVrEYKZgz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=True,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)"
      ],
      "metadata": {
        "id": "uHd9ell_Mm33"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not all arguments here are useful, the sentence piece tokenizer has a historical context - it sees the raw stream of characters as sentences.hence the max_bytes per sentence, num_sentences etc. But according to Karpathy, it is better to keep the raw buffer as raw buffer itself -> just a stream of characters which are to be converted to tokens."
      ],
      "metadata": {
        "id": "lGBjhoIuMobW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]"
      ],
      "metadata": {
        "id": "PIZKacZEObhf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TggFlR5MO6ri",
        "outputId": "f522b52c-f465-4e26-efca-b7f851c86d44"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['en', 259],\n",
              " ['‚ñÅt', 260],\n",
              " ['ce', 261],\n",
              " ['in', 262],\n",
              " ['ra', 263],\n",
              " ['‚ñÅa', 264],\n",
              " ['de', 265],\n",
              " ['er', 266],\n",
              " ['‚ñÅs', 267],\n",
              " ['ent', 268],\n",
              " ['or', 269],\n",
              " ['pr', 270],\n",
              " ['‚ñÅm', 271],\n",
              " ['‚ñÅu', 272],\n",
              " ['ing', 273],\n",
              " ['‚ñÅth', 274],\n",
              " ['ence', 275],\n",
              " ['entence', 276],\n",
              " ['Pi', 277],\n",
              " ['ed', 278],\n",
              " ['em', 279],\n",
              " ['ex', 280],\n",
              " ['is', 281],\n",
              " ['iz', 282],\n",
              " ['la', 283],\n",
              " ['on', 284],\n",
              " ['st', 285],\n",
              " ['‚ñÅS', 286],\n",
              " ['Pie', 287],\n",
              " ['end', 288],\n",
              " ['ext', 289],\n",
              " ['‚ñÅan', 290],\n",
              " ['‚ñÅpr', 291],\n",
              " ['‚ñÅto', 292],\n",
              " ['‚ñÅun', 293],\n",
              " ['‚ñÅthe', 294],\n",
              " ['Piece', 295],\n",
              " ['‚ñÅSentence', 296],\n",
              " ['‚ñÅSentencePiece', 297],\n",
              " ['.]', 298],\n",
              " ['Ne', 299],\n",
              " ['ag', 300],\n",
              " ['do', 301],\n",
              " ['ec', 302],\n",
              " ['gu', 303],\n",
              " ['ic', 304],\n",
              " ['ir', 305],\n",
              " ['it', 306],\n",
              " ['ly', 307],\n",
              " ['to', 308],\n",
              " ['‚ñÅ(', 309],\n",
              " ['‚ñÅ[', 310],\n",
              " ['‚ñÅf', 311],\n",
              " ['‚ñÅn', 312],\n",
              " ['‚ñÅw', 313],\n",
              " ['.])', 314],\n",
              " ['age', 315],\n",
              " ['del', 316],\n",
              " ['ion', 317],\n",
              " ['ken', 318],\n",
              " ['lan', 319],\n",
              " ['ral', 320],\n",
              " ['wor', 321],\n",
              " ['yst', 322],\n",
              " ['‚ñÅNe', 323],\n",
              " ['‚ñÅal', 324],\n",
              " ['‚ñÅde', 325],\n",
              " ['‚ñÅis', 326],\n",
              " ['‚ñÅma', 327],\n",
              " ['‚ñÅmo', 328],\n",
              " ['izer', 329],\n",
              " ['rain', 330],\n",
              " ['ural', 331],\n",
              " ['‚ñÅand', 332],\n",
              " ['‚ñÅlan', 333],\n",
              " ['‚ñÅpre', 334],\n",
              " ['guage', 335],\n",
              " ['ystem', 336],\n",
              " ['‚ñÅtext', 337],\n",
              " ['‚ñÅmodel', 338],\n",
              " ['‚ñÅtrain', 339],\n",
              " ['kenizer', 340],\n",
              " ['‚ñÅsystem', 341],\n",
              " ['‚ñÅlanguage', 342],\n",
              " ['‚ñÅtraining', 343],\n",
              " ['.,', 344],\n",
              " ['BP', 345],\n",
              " ['Ku', 346],\n",
              " ['ab', 347],\n",
              " ['as', 348],\n",
              " ['at', 349],\n",
              " ['by', 350],\n",
              " ['co', 351],\n",
              " ['es', 352],\n",
              " ['et', 353],\n",
              " ['if', 354],\n",
              " ['ig', 355],\n",
              " ['im', 356],\n",
              " ['ke', 357],\n",
              " ['lo', 358],\n",
              " ['nr', 359],\n",
              " ['oc', 360],\n",
              " ['e', 361],\n",
              " ['‚ñÅ', 362],\n",
              " ['n', 363],\n",
              " ['t', 364],\n",
              " ['i', 365],\n",
              " ['r', 366],\n",
              " ['a', 367],\n",
              " ['o', 368],\n",
              " ['s', 369],\n",
              " ['d', 370],\n",
              " ['c', 371],\n",
              " ['l', 372],\n",
              " ['u', 373],\n",
              " ['g', 374],\n",
              " ['m', 375],\n",
              " ['p', 376],\n",
              " ['.', 377],\n",
              " ['h', 378],\n",
              " ['-', 379],\n",
              " ['w', 380],\n",
              " ['y', 381],\n",
              " ['P', 382],\n",
              " ['S', 383],\n",
              " ['b', 384],\n",
              " ['f', 385],\n",
              " ['k', 386],\n",
              " [')', 387],\n",
              " ['x', 388],\n",
              " ['z', 389],\n",
              " ['(', 390],\n",
              " ['N', 391],\n",
              " ['[', 392],\n",
              " [']', 393],\n",
              " ['v', 394],\n",
              " [',', 395],\n",
              " ['/', 396],\n",
              " ['B', 397],\n",
              " ['E', 398],\n",
              " ['K', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as seen above, since byte_format was given as true,we have the special tokens first , then the bytecode tokens, then the unicode tokens encoded directly with merges."
      ],
      "metadata": {
        "id": "rm11D6VkPGK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî\")"
      ],
      "metadata": {
        "id": "QV0UiXLAO8hj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfPXShflPxv8",
        "outputId": "1e1a4562-15e5-4683-a51e-71d0405b8cd8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the korean tokens are new and not encoutered in training set. so, they should have been unk tokens. but we have activated utf-8 bytecode tokens as well. so, the these korean letters have corresponding bytecode patterns which are part of vocab and those are printed"
      ],
      "metadata": {
        "id": "RBoKUPTYP3lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([sp.id_to_piece(idx) for idx in ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5TFWlhkPzy4",
        "outputId": "8b7aedb8-8ea1-40f8-e237-2e756597a800"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‚ñÅ', 'h', 'e', 'l', 'lo', '‚ñÅ', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we use byte_fallback - False, the korean lettes would be giveb the unknown token and this will limit the capabilities of the model."
      ],
      "metadata": {
        "id": "pSu8V5QXQp0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ptions = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=False, # to demonstrate the problems this would cause in korean case\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)"
      ],
      "metadata": {
        "id": "Q_iJxNytQWjk"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]"
      ],
      "metadata": {
        "id": "e3L-MqXRQ_rT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN-dHVecRd19",
        "outputId": "adcf3f68-3282-4841-e1d8-282236591f01"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['en', 259],\n",
              " ['‚ñÅt', 260],\n",
              " ['ce', 261],\n",
              " ['in', 262],\n",
              " ['ra', 263],\n",
              " ['‚ñÅa', 264],\n",
              " ['de', 265],\n",
              " ['er', 266],\n",
              " ['‚ñÅs', 267],\n",
              " ['ent', 268],\n",
              " ['or', 269],\n",
              " ['pr', 270],\n",
              " ['‚ñÅm', 271],\n",
              " ['‚ñÅu', 272],\n",
              " ['ing', 273],\n",
              " ['‚ñÅth', 274],\n",
              " ['ence', 275],\n",
              " ['entence', 276],\n",
              " ['Pi', 277],\n",
              " ['ed', 278],\n",
              " ['em', 279],\n",
              " ['ex', 280],\n",
              " ['is', 281],\n",
              " ['iz', 282],\n",
              " ['la', 283],\n",
              " ['on', 284],\n",
              " ['st', 285],\n",
              " ['‚ñÅS', 286],\n",
              " ['Pie', 287],\n",
              " ['end', 288],\n",
              " ['ext', 289],\n",
              " ['‚ñÅan', 290],\n",
              " ['‚ñÅpr', 291],\n",
              " ['‚ñÅto', 292],\n",
              " ['‚ñÅun', 293],\n",
              " ['‚ñÅthe', 294],\n",
              " ['Piece', 295],\n",
              " ['‚ñÅSentence', 296],\n",
              " ['‚ñÅSentencePiece', 297],\n",
              " ['.]', 298],\n",
              " ['Ne', 299],\n",
              " ['ag', 300],\n",
              " ['do', 301],\n",
              " ['ec', 302],\n",
              " ['gu', 303],\n",
              " ['ic', 304],\n",
              " ['ir', 305],\n",
              " ['it', 306],\n",
              " ['ly', 307],\n",
              " ['to', 308],\n",
              " ['‚ñÅ(', 309],\n",
              " ['‚ñÅ[', 310],\n",
              " ['‚ñÅf', 311],\n",
              " ['‚ñÅn', 312],\n",
              " ['‚ñÅw', 313],\n",
              " ['.])', 314],\n",
              " ['age', 315],\n",
              " ['del', 316],\n",
              " ['ion', 317],\n",
              " ['ken', 318],\n",
              " ['lan', 319],\n",
              " ['ral', 320],\n",
              " ['wor', 321],\n",
              " ['yst', 322],\n",
              " ['‚ñÅNe', 323],\n",
              " ['‚ñÅal', 324],\n",
              " ['‚ñÅde', 325],\n",
              " ['‚ñÅis', 326],\n",
              " ['‚ñÅma', 327],\n",
              " ['‚ñÅmo', 328],\n",
              " ['izer', 329],\n",
              " ['rain', 330],\n",
              " ['ural', 331],\n",
              " ['‚ñÅand', 332],\n",
              " ['‚ñÅlan', 333],\n",
              " ['‚ñÅpre', 334],\n",
              " ['guage', 335],\n",
              " ['ystem', 336],\n",
              " ['‚ñÅtext', 337],\n",
              " ['‚ñÅmodel', 338],\n",
              " ['‚ñÅtrain', 339],\n",
              " ['kenizer', 340],\n",
              " ['‚ñÅsystem', 341],\n",
              " ['‚ñÅlanguage', 342],\n",
              " ['‚ñÅtraining', 343],\n",
              " ['.,', 344],\n",
              " ['BP', 345],\n",
              " ['Ku', 346],\n",
              " ['ab', 347],\n",
              " ['as', 348],\n",
              " ['at', 349],\n",
              " ['by', 350],\n",
              " ['co', 351],\n",
              " ['es', 352],\n",
              " ['et', 353],\n",
              " ['if', 354],\n",
              " ['ig', 355],\n",
              " ['im', 356],\n",
              " ['ke', 357],\n",
              " ['lo', 358],\n",
              " ['nr', 359],\n",
              " ['oc', 360],\n",
              " ['e', 361],\n",
              " ['‚ñÅ', 362],\n",
              " ['n', 363],\n",
              " ['t', 364],\n",
              " ['i', 365],\n",
              " ['r', 366],\n",
              " ['a', 367],\n",
              " ['o', 368],\n",
              " ['s', 369],\n",
              " ['d', 370],\n",
              " ['c', 371],\n",
              " ['l', 372],\n",
              " ['u', 373],\n",
              " ['g', 374],\n",
              " ['m', 375],\n",
              " ['p', 376],\n",
              " ['.', 377],\n",
              " ['h', 378],\n",
              " ['-', 379],\n",
              " ['w', 380],\n",
              " ['y', 381],\n",
              " ['P', 382],\n",
              " ['S', 383],\n",
              " ['b', 384],\n",
              " ['f', 385],\n",
              " ['k', 386],\n",
              " [')', 387],\n",
              " ['x', 388],\n",
              " ['z', 389],\n",
              " ['(', 390],\n",
              " ['N', 391],\n",
              " ['[', 392],\n",
              " [']', 393],\n",
              " ['v', 394],\n",
              " [',', 395],\n",
              " ['/', 396],\n",
              " ['B', 397],\n",
              " ['E', 398],\n",
              " ['K', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî\")"
      ],
      "metadata": {
        "id": "c3glAys9RK4i"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1wyjGpgRMTk",
        "outputId": "3e9bf154-04ff-4466-8d43-cd167ff7daac"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[362,\n",
              " 378,\n",
              " 361,\n",
              " 372,\n",
              " 358,\n",
              " 362,\n",
              " 239,\n",
              " 152,\n",
              " 139,\n",
              " 238,\n",
              " 136,\n",
              " 152,\n",
              " 240,\n",
              " 152,\n",
              " 155,\n",
              " 239,\n",
              " 135,\n",
              " 187,\n",
              " 239,\n",
              " 157,\n",
              " 151]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a _ before the start of the sentence i.e before hello world is because of setting dummy_prefix = True"
      ],
      "metadata": {
        "id": "7icPQ5QfSW9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "what should be vocab_size ?\n",
        "in GPT code, the vocab_size is used in two places . the place where the embedding table is constructed and the final place where there is alinear layer which predicts the probabilitites for all vocab sizes.\n",
        "with infinite or very large vocab size, the computations associated with final layer increase, the embedding table becoems bigger and num of paramters also increase."
      ],
      "metadata": {
        "id": "I67QDL34TPyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To change the vocab size say to increase the number of tokens, the base model is frozen and the only the layers associated with vocab are changed. a small model surgery is enough."
      ],
      "metadata": {
        "id": "UuYnF15yUL7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also another optimization rather compression which is possible with tokenization - we take a chat language model like chatGPT and it is usually finetuend with prompt engineering, so instead of that, the base model is taken and a few more tokens are added and the other layers are frozen. the model is distilled to improve it performance without the need of prompts."
      ],
      "metadata": {
        "id": "Md0-oIdRVqww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many problems associated with tokenization : in python due to spaces considered as separate tokens, it was not able to attend to many tokens. so,it was fixed in gpt4.\n",
        "And non-English languages are not modelled properly.\n",
        "not enough data to train and tokenization of non-English languages are expensive ,more tokens for semantically similar words.\n",
        "potential attack from user end having knowledge of special tokens. |<end_of_text>| if passed , the processing abrupty stop for chatgpt.\n",
        "Integer numbers are all tokenized with bizzare merges. instead of taking one 4 digit number as single token, it is split as 3-1 , 2-2, 1-3"
      ],
      "metadata": {
        "id": "9UHW2MMagdgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain strings like SolidGoldMagiKarp which disturb the LLMs and make them go bizzare. they interpret them differently and produce unexpected outputs\n"
      ],
      "metadata": {
        "id": "pfiAsaWsjm_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "how this happens is that, solidmagikarp is a reddit user account who is very active. and vocab was created with this dataset. so, there was a dedicated token for this user which was then never encountered during training. the result was that the embedding row vector for this token was untrained and was junk.\n",
        "When a user tries to infer with this token, it junk passes forward into the transformer block and it gives rise to weird outputs\n"
      ],
      "metadata": {
        "id": "Jr5dyNuQldAw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Gj8l4zLRMka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}