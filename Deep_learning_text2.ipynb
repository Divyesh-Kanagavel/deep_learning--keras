{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil, pathlib\n",
    "base_dir = pathlib.Path(\"/Users/divyeshkanagavel/Desktop/DeepLearning/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\",\"pos\"):\n",
    "  os.makedirs(val_dir / category)\n",
    "  files = os.listdir(train_dir / category)\n",
    "  random.Random(1337).shuffle(files)\n",
    "  num_val_samples = int(0.2*len(files))\n",
    "  val_files = files[-num_val_samples:]\n",
    "  for fname in val_files:\n",
    "    shutil.move(train_dir/category/fname, val_dir/category/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory('/Users/divyeshkanagavel/Desktop/DeepLearning/aclImdb/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 600 # truncate length of reviews to 600\n",
    "max_tokens = 20000 # max tokens in vocab\n",
    "text_vectorization = layers.TextVectorization(max_tokens = max_tokens, output_mode = \"int\", output_sequence_length=max_length)\n",
    "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds = train_ds.map(\n",
    "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_24 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5194049 (19.81 MB)\n",
      "Trainable params: 5194049 (19.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\") \n",
    "embedded = layers.Embedding(\n",
    "input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs) \n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs) \n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "loss=\"binary_crossentropy\",\n",
    "metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check fit function call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 91s 140ms/step - loss: 0.6948 - accuracy: 0.5059 - val_loss: 0.6924 - val_accuracy: 0.5072\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 81s 129ms/step - loss: 0.6918 - accuracy: 0.5012 - val_loss: 0.6897 - val_accuracy: 0.5128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x577074310>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(key_dim = embed_dim, num_heads = num_heads)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim,activation=\"relu\"),\n",
    "                                            layers.Dense(embed_dim),])\n",
    "        self.layer_norm1 = layers.LayerNormalization()\n",
    "        self.layer_norm2 = layers.LayerNormalization()\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:,tf.newaxis,:]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask = mask)\n",
    "        proj_input = self.layer_norm1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layer_norm2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\" : self.embed_dim,\n",
    "            \"dense_dim\" : self.dense_dim,\n",
    "            \"num_heads\" : self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inputs)\n",
    "#mask = layers.Embedding(vocab_size, embed_dim, mask_zero=True).compute_mask(inputs)\n",
    "\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) \n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = keras.callbacks.ModelCheckpoint(\"transformer_encoder_classifier.h5\",save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 277s 441ms/step - loss: 0.4278 - accuracy: 0.8123 - val_loss: 0.3121 - val_accuracy: 0.8780\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 274s 438ms/step - loss: 0.2378 - accuracy: 0.9057 - val_loss: 0.2408 - val_accuracy: 0.9018\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 1227s 2s/step - loss: 0.1799 - accuracy: 0.9336 - val_loss: 0.3012 - val_accuracy: 0.8954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x32dbc1f50>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(int_train_ds, validation_data = int_val_ds, epochs = 3,callbacks = callbacks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 134s 171ms/step - loss: 0.2868 - accuracy: 0.8835\n",
      "Test acc: 0.884\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder_classifier.h5\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a test accuracy of 88.4 percent is observed which is pretty cool , an improvement over GRU model emphasising the importance of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type='GPU'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is something missing here! the tokens are interacting with all other tokens, updating the values for each token thereby and then dense network is learning task specific features. but, self attention by itself has no mechanism for tracking word order -> this is inherently possible in architectures like RNN, LSTM etc. transformer adds additional positional encoding to self attention to truly become a sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words are already embedded into a vector depending on the semantic relationship between them.to this vector we add a quantity which will give positional information as well.simplest technique could to add the word position index in the sentence, but then this will be a huge integer value for long sequences adn neural networks work well with values in the range [-1,1]. the original paper used the cosine embedding where the word index are converted to values in the range [-1,1] by encoding word index into cosine function. \n",
    "or the way we are going to do is to use the neural network gradients to learn correct positional encoding values to be added to word embeddings ->this technique is called positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self,sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embedding = layers.Embedding(input_dim=input_dim, output_dim = output_dim)\n",
    "        self.position_embedding = layers.Embedding(input_dim = sequence_length, output_dim = output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0,limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embedding(inputs)\n",
    "        embedded_positions = self.position_embedding(positions)\n",
    "        return (embedded_tokens+embedded_positions)\n",
    "    \n",
    "    def compute_mask(self,inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\":self.input_dim,\n",
    "            \"output_dim\":self.output_dim,\n",
    "            \"sequence_length\":self.sequence_length\n",
    "\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads) (x)# mask from position embedding is propagated to the following layers\n",
    "x = layers.GlobalMaxPooling1D() (x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\") (x)\n",
    "model = keras.Model(inputs = inputs , outputs= outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.h5\",\n",
    "                                    save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 288s 458ms/step - loss: 0.4759 - accuracy: 0.7794 - val_loss: 0.2642 - val_accuracy: 0.8850\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 277s 444ms/step - loss: 0.2456 - accuracy: 0.9021 - val_loss: 0.2812 - val_accuracy: 0.8992\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 285s 456ms/step - loss: 0.1845 - accuracy: 0.9308 - val_loss: 0.5217 - val_accuracy: 0.8308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x728450810>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=3,\n",
    "     callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 136s 173ms/step - loss: 0.2921 - accuracy: 0.8765\n",
      "Test acc: 0.877\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.h5\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "\"PositionEmbedding\": PositionEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is more or less the same. the bag of words approach also gives a good accuracy for this dataset and is faster as well.\n",
    "there is a general rule of thumb observed by keras research group for nlp classification task -> if the number of samples / mean number of words per sample < 1500, go for bag of words model with dense neural networks, else go for sequence models like transformer , GRU etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#a more complex application : Sequence-sequence model\n",
    "1.machine translation\n",
    "2.Summarization\n",
    "3. Text generation : convert a text prompt into paragraph\n",
    "4. Question answering \n",
    "5. Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "machine translation by transformers is done using both the encoder and the decoder\n",
    "Encoder: An encoder model turns the source sequence into an intermediate representation.\n",
    "Decoder : the decoder model predicts token i given token i-1 and the intermediate representation from encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
