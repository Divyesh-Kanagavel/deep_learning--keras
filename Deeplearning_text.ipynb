{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyesh-Kanagavel/deep_learning--keras/blob/master/Deeplearning_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dasaNQHgjPl7"
      },
      "source": [
        "Classic nlp steps:\n",
        "standardization of text : depending on text remove unwanted characters,\n",
        "punctuations, lowercase everything etc\n",
        "Tokenization : break the text into tokens [bag of words] or a sequence of words into words / sub-words.\n",
        "N-gram approach is useful when we dont need to care about sequence , but it stores some info on local ordering of words like \" on the\" , \"to go\" etc . for applications where sequence is important -> word-wise tokenizations is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apI-bQh0j9uj"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "class Vectorizer:\n",
        "  def standardize(self,text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "  def tokenize(self,text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "  def make_vocabulary(self,dataset):\n",
        "    self.vocabulary = {\"\" : 0, \"UNK\" : 1}\n",
        "    for text in dataset:\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.inverse_vocabulary = dict((v,k) for k,v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self,text):\n",
        "   tokens = self.tokenize(text)\n",
        "   return [self.vocabulary.get(token,1) for token in tokens]\n",
        "  def decode(self,int_sequence):\n",
        "   return \" \".join(self.inverse_vocabulary.get(i,\"UNK\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\"I write, erase, rewrite\", \"Erase again, and then\",\"A poppy blossoms.\" ]\n",
        "vectorizer.make_vocabulary(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLVOeNHYmqfl",
        "outputId": "9e0d45e8-a179-4116-8353-774da8768110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n",
            "i write rewrite and UNK rewrite again\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-RJy6IJnMHn"
      },
      "source": [
        "Using pure python code will not be performant and it is better to use the optimal keras library for the same . the benefit of using keras is that it can be integrated to tf.data pipeline easily. also,the standardisation and tokenization functions can be customized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aCBNXrlSzb4k"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cua3a2NOunnQ"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(output_mode = 'int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqRlyq06wSrI",
        "outputId": "747adb8b-bd1e-4cfe-8cab-ce076da84ea8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#a sample dataset - to illustrate how vocab is created and how tokenization is done with a pre-available dataset\n",
        "dataset = [\"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",]\n",
        "text_vectorization.adapt(dataset) # api for vectorization object to use this dataset as base for developing vocab\n",
        "text_vectorization.get_vocabulary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It2Q-6Frwn26",
        "outputId": "b6cba788-7538-4c16-92a5-b5a465605f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  9  4  2  9  4  3 10  6  8], shape=(11,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = 'I write and then erase and then write again poppy blooms'\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywH7M9zMxAfd",
        "outputId": "036754a2-63fc-4440-fb35-bb214b260b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '', 1: '[UNK]', 2: 'erase', 3: 'write', 4: 'then', 5: 'rewrite', 6: 'poppy', 7: 'i', 8: 'blooms', 9: 'and', 10: 'again', 11: 'a'}\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "print(inverse_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "586PqEAAxPqy"
      },
      "outputs": [],
      "source": [
        "decoded_sentence = ' '.join(inverse_vocab[int(i)] for i in encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9hzKNah3xggF",
        "outputId": "498ed77d-67e0-42f9-d2d7-47ecb395c39c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i write and then erase and then write again poppy blooms'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swim5ZMm1i6M"
      },
      "source": [
        "text_vectorization is only a dictionary lookup operation and is done on the and not on gpu/tpu.\n",
        "hence, if this is added inside a keras model as a functional api , the rest of the model will be on gpu and will wait till the lookup is done on the cpu at each training cycle.the transfoer of data between cpu and gpu will be expensive (synchronous operation)\n",
        "if this function is added in to tf.data pipeline, the text_vectorization api can be processed on batches of data asynchronously spread across multiple cores making it efficient\n",
        "e.g.\n",
        "int_sequence_dataset = string_dataset.map(text_vectorization, parallel_calls=4) -> spread across multiple cores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ptzYsj8UWl"
      },
      "source": [
        "Transformers and rnns are sequence models. there are two approaches to solve natural language problems\n",
        "either discard order and treat words in a text as bag of words and process the data, or treat the words strictly based on the incoming order like steps in a timeseries and put them into rnn sort of models.\n",
        "Transformers are order agnostic but in their representations, they encode some sort of word positions information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/aclImdb"
      ],
      "metadata": {
        "id": "SL20UZ-7MPTN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK1zt_QCxoTx",
        "outputId": "6677eaf1-dddc-42c1-c00d-ecbb7ca6ecca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  27.4M      0  0:00:02  0:00:02 --:--:-- 27.4M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "KGmSB_AhjnW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/aclImdb/train/pos/10000_8.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHSuW0n5kA5m",
        "outputId": "f9523dab-120e-4472-8f10-f8d75f94834c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, shutil, pathlib\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\",\"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2*len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir/category/fname, val_dir/category/fname)\n"
      ],
      "metadata": {
        "id": "2Hc1cbCDkPt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "train_ds = keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory('aclImdb/test', batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Mhe6Rg5Smorm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are many different ways of building the input tensors -> discard order and form a set of words in the input text. then hot encode them thus getting a single vector with zeros in other word positions and ones in words which are there in the text.\n",
        "look at one word at a time - unigram or form sets of n consecutive words - n-gram thereby preserving local order information.\n",
        "first basic model is unigram with a single multidimensional n-hot encoded vector passed to a neural network.\n",
        "limit the vocab to 20,000 common words -> this is a heuristic, usually 20000 works well."
      ],
      "metadata": {
        "id": "3Cn8eAc1B8xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(max_tokens = 20000, output_mode = 'multi_hot')"
      ],
      "metadata": {
        "id": "rpGCFx5R0WWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n"
      ],
      "metadata": {
        "id": "QVTFg6pjYZCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmY2BCeVnB5F",
        "outputId": "87ed50f2-0663-4908-8356-bedb17c8e188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " 'the',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'it',\n",
              " 'i',\n",
              " 'this',\n",
              " 'that',\n",
              " 'br',\n",
              " 'was',\n",
              " 'as',\n",
              " 'with',\n",
              " 'for',\n",
              " 'but',\n",
              " 'movie',\n",
              " 'film',\n",
              " 'on',\n",
              " 'not',\n",
              " 'you',\n",
              " 'his',\n",
              " 'are',\n",
              " 'have',\n",
              " 'he',\n",
              " 'be',\n",
              " 'one',\n",
              " 'its',\n",
              " 'at',\n",
              " 'all',\n",
              " 'by',\n",
              " 'an',\n",
              " 'they',\n",
              " 'from',\n",
              " 'who',\n",
              " 'so',\n",
              " 'like',\n",
              " 'her',\n",
              " 'or',\n",
              " 'just',\n",
              " 'about',\n",
              " 'has',\n",
              " 'if',\n",
              " 'out',\n",
              " 'some',\n",
              " 'there',\n",
              " 'what',\n",
              " 'good',\n",
              " 'more',\n",
              " 'very',\n",
              " 'when',\n",
              " 'even',\n",
              " 'my',\n",
              " 'she',\n",
              " 'no',\n",
              " 'up',\n",
              " 'would',\n",
              " 'which',\n",
              " 'time',\n",
              " 'only',\n",
              " 'really',\n",
              " 'story',\n",
              " 'their',\n",
              " 'were',\n",
              " 'see',\n",
              " 'had',\n",
              " 'can',\n",
              " 'me',\n",
              " 'than',\n",
              " 'we',\n",
              " 'much',\n",
              " 'well',\n",
              " 'been',\n",
              " 'get',\n",
              " 'also',\n",
              " 'into',\n",
              " 'will',\n",
              " 'other',\n",
              " 'do',\n",
              " 'great',\n",
              " 'bad',\n",
              " 'people',\n",
              " 'because',\n",
              " 'first',\n",
              " 'most',\n",
              " 'how',\n",
              " 'him',\n",
              " 'dont',\n",
              " 'made',\n",
              " 'then',\n",
              " 'movies',\n",
              " 'could',\n",
              " 'films',\n",
              " 'make',\n",
              " 'way',\n",
              " 'any',\n",
              " 'after',\n",
              " 'too',\n",
              " 'them',\n",
              " 'characters',\n",
              " 'think',\n",
              " 'watch',\n",
              " 'many',\n",
              " 'two',\n",
              " 'being',\n",
              " 'seen',\n",
              " 'little',\n",
              " 'character',\n",
              " 'never',\n",
              " 'best',\n",
              " 'plot',\n",
              " 'where',\n",
              " 'acting',\n",
              " 'did',\n",
              " 'love',\n",
              " 'know',\n",
              " 'show',\n",
              " 'life',\n",
              " 'does',\n",
              " 'ever',\n",
              " 'your',\n",
              " 'still',\n",
              " 'better',\n",
              " 'over',\n",
              " 'off',\n",
              " 'end',\n",
              " 'these',\n",
              " 'while',\n",
              " 'say',\n",
              " 'why',\n",
              " 'here',\n",
              " 'scenes',\n",
              " 'man',\n",
              " 'scene',\n",
              " 'such',\n",
              " 'go',\n",
              " 'should',\n",
              " 'through',\n",
              " 'something',\n",
              " 'back',\n",
              " 'im',\n",
              " 'those',\n",
              " 'doesnt',\n",
              " 'watching',\n",
              " 'years',\n",
              " 'real',\n",
              " 'though',\n",
              " 'thing',\n",
              " 'now',\n",
              " 'didnt',\n",
              " 'actors',\n",
              " 'nothing',\n",
              " 'another',\n",
              " 'new',\n",
              " 'actually',\n",
              " 'makes',\n",
              " 'before',\n",
              " 'look',\n",
              " 'work',\n",
              " 'find',\n",
              " 'funny',\n",
              " 'few',\n",
              " 'old',\n",
              " 'same',\n",
              " 'going',\n",
              " 'every',\n",
              " 'us',\n",
              " 'lot',\n",
              " 'part',\n",
              " 'director',\n",
              " 'thats',\n",
              " 'quite',\n",
              " 'cast',\n",
              " 'cant',\n",
              " 'again',\n",
              " 'pretty',\n",
              " 'want',\n",
              " 'young',\n",
              " 'things',\n",
              " 'got',\n",
              " 'seems',\n",
              " 'around',\n",
              " 'fact',\n",
              " 'down',\n",
              " 'however',\n",
              " 'enough',\n",
              " 'world',\n",
              " 'between',\n",
              " 'take',\n",
              " 'give',\n",
              " 'both',\n",
              " 'original',\n",
              " 'own',\n",
              " 'big',\n",
              " 'may',\n",
              " 'ive',\n",
              " 'horror',\n",
              " 'thought',\n",
              " 'gets',\n",
              " 'always',\n",
              " 'right',\n",
              " 'isnt',\n",
              " 'without',\n",
              " 'long',\n",
              " 'times',\n",
              " 'come',\n",
              " 'series',\n",
              " 'saw',\n",
              " 'almost',\n",
              " 'interesting',\n",
              " 'action',\n",
              " 'role',\n",
              " 'least',\n",
              " 'theres',\n",
              " 'whole',\n",
              " 'must',\n",
              " 'family',\n",
              " 'comedy',\n",
              " 'point',\n",
              " 'bit',\n",
              " 'script',\n",
              " 'music',\n",
              " 'done',\n",
              " 'far',\n",
              " 'since',\n",
              " 'might',\n",
              " 'guy',\n",
              " 'probably',\n",
              " 'anything',\n",
              " 'last',\n",
              " 'feel',\n",
              " 'minutes',\n",
              " 'hes',\n",
              " 'performance',\n",
              " 'am',\n",
              " 'rather',\n",
              " 'kind',\n",
              " 'yet',\n",
              " 'worst',\n",
              " 'away',\n",
              " 'making',\n",
              " 'sure',\n",
              " 'girl',\n",
              " 'woman',\n",
              " 'anyone',\n",
              " 'tv',\n",
              " 'found',\n",
              " 'fun',\n",
              " 'played',\n",
              " 'although',\n",
              " 'each',\n",
              " 'our',\n",
              " 'having',\n",
              " 'comes',\n",
              " 'course',\n",
              " 'believe',\n",
              " 'trying',\n",
              " 'looks',\n",
              " 'hard',\n",
              " 'goes',\n",
              " 'especially',\n",
              " 'day',\n",
              " 'shows',\n",
              " 'wasnt',\n",
              " 'different',\n",
              " 'put',\n",
              " 'maybe',\n",
              " 'place',\n",
              " 'main',\n",
              " 'set',\n",
              " 'reason',\n",
              " 'once',\n",
              " 'sense',\n",
              " 'plays',\n",
              " 'ending',\n",
              " 'worth',\n",
              " 'true',\n",
              " 'everything',\n",
              " 'money',\n",
              " 'watched',\n",
              " '2',\n",
              " 'looking',\n",
              " 'someone',\n",
              " 'seem',\n",
              " 'actor',\n",
              " 'dvd',\n",
              " 'said',\n",
              " 'job',\n",
              " 'instead',\n",
              " 'together',\n",
              " 'version',\n",
              " 'three',\n",
              " 'beautiful',\n",
              " '10',\n",
              " 'takes',\n",
              " 'screen',\n",
              " 'john',\n",
              " 'later',\n",
              " 'play',\n",
              " 'himself',\n",
              " 'book',\n",
              " 'during',\n",
              " 'effects',\n",
              " 'night',\n",
              " 'excellent',\n",
              " 'left',\n",
              " 'seeing',\n",
              " 'everyone',\n",
              " 'nice',\n",
              " 'special',\n",
              " 'american',\n",
              " 'house',\n",
              " 'audience',\n",
              " 'idea',\n",
              " 'wife',\n",
              " 'simply',\n",
              " 'youre',\n",
              " 'less',\n",
              " 'shot',\n",
              " 'high',\n",
              " 'black',\n",
              " 'read',\n",
              " 'completely',\n",
              " 'second',\n",
              " 'else',\n",
              " 'used',\n",
              " 'star',\n",
              " 'year',\n",
              " 'fan',\n",
              " 'war',\n",
              " 'father',\n",
              " 'kids',\n",
              " 'help',\n",
              " 'given',\n",
              " 'death',\n",
              " 'try',\n",
              " 'poor',\n",
              " 'need',\n",
              " 'use',\n",
              " 'friends',\n",
              " 'mind',\n",
              " 'until',\n",
              " 'short',\n",
              " 'rest',\n",
              " 'home',\n",
              " 'men',\n",
              " 'enjoy',\n",
              " 'either',\n",
              " 'along',\n",
              " 'classic',\n",
              " 'performances',\n",
              " 'dead',\n",
              " 'hollywood',\n",
              " 'line',\n",
              " 'half',\n",
              " 'wrong',\n",
              " 'truly',\n",
              " 'tell',\n",
              " 'remember',\n",
              " 'boring',\n",
              " 'understand',\n",
              " 'production',\n",
              " 'next',\n",
              " 'recommend',\n",
              " 'came',\n",
              " 'start',\n",
              " 'women',\n",
              " 'perhaps',\n",
              " 'full',\n",
              " 'couple',\n",
              " 'stupid',\n",
              " 'let',\n",
              " 'definitely',\n",
              " 'others',\n",
              " 'camera',\n",
              " 'wonderful',\n",
              " 'playing',\n",
              " 'terrible',\n",
              " 'keep',\n",
              " 'mean',\n",
              " 'stars',\n",
              " 'moments',\n",
              " 'episode',\n",
              " 'small',\n",
              " 'often',\n",
              " 'gives',\n",
              " 'awful',\n",
              " 'getting',\n",
              " 'felt',\n",
              " 'perfect',\n",
              " 'early',\n",
              " 'lines',\n",
              " 'human',\n",
              " 'face',\n",
              " 'become',\n",
              " 'doing',\n",
              " 'sex',\n",
              " 'video',\n",
              " 'finally',\n",
              " 'liked',\n",
              " 'name',\n",
              " 'person',\n",
              " 'supposed',\n",
              " 'piece',\n",
              " 'lost',\n",
              " 'yes',\n",
              " 'school',\n",
              " 'went',\n",
              " 'couldnt',\n",
              " 'live',\n",
              " 'entire',\n",
              " 'dialogue',\n",
              " 'case',\n",
              " 'top',\n",
              " 'absolutely',\n",
              " 'certainly',\n",
              " 'itself',\n",
              " 'against',\n",
              " 'sort',\n",
              " 'shes',\n",
              " 'waste',\n",
              " 'style',\n",
              " 'title',\n",
              " 'written',\n",
              " 'head',\n",
              " 'problem',\n",
              " 'loved',\n",
              " 'evil',\n",
              " 'several',\n",
              " 'budget',\n",
              " 'overall',\n",
              " 'hope',\n",
              " 'mr',\n",
              " 'id',\n",
              " 'despite',\n",
              " 'boy',\n",
              " 'worse',\n",
              " 'white',\n",
              " 'picture',\n",
              " 'beginning',\n",
              " 'entertaining',\n",
              " '3',\n",
              " 'care',\n",
              " 'fans',\n",
              " 'dark',\n",
              " 'cinema',\n",
              " 'wanted',\n",
              " 'seemed',\n",
              " 'oh',\n",
              " 'mother',\n",
              " 'killer',\n",
              " 'based',\n",
              " 'already',\n",
              " 'lives',\n",
              " 'becomes',\n",
              " 'final',\n",
              " 'throughout',\n",
              " 'guys',\n",
              " 'example',\n",
              " 'direction',\n",
              " 'unfortunately',\n",
              " 'fine',\n",
              " 'girls',\n",
              " 'amazing',\n",
              " 'guess',\n",
              " 'sound',\n",
              " 'humor',\n",
              " 'turn',\n",
              " 'wont',\n",
              " 'wants',\n",
              " 'totally',\n",
              " 'low',\n",
              " 'children',\n",
              " '1',\n",
              " 'friend',\n",
              " 'history',\n",
              " 'laugh',\n",
              " 'called',\n",
              " 'tries',\n",
              " '\\x96',\n",
              " 'drama',\n",
              " 'son',\n",
              " 'able',\n",
              " 'past',\n",
              " 'lead',\n",
              " 'youll',\n",
              " 'works',\n",
              " 'theyre',\n",
              " 'writing',\n",
              " 'turns',\n",
              " 'gave',\n",
              " 'michael',\n",
              " 'enjoyed',\n",
              " 'quality',\n",
              " 'under',\n",
              " 'kill',\n",
              " 'behind',\n",
              " 'viewer',\n",
              " 'favorite',\n",
              " 'days',\n",
              " 'sometimes',\n",
              " 'act',\n",
              " 'starts',\n",
              " 'game',\n",
              " 'town',\n",
              " 'child',\n",
              " 'horrible',\n",
              " 'expect',\n",
              " 'car',\n",
              " 'eyes',\n",
              " 'soon',\n",
              " 'ones',\n",
              " 'side',\n",
              " 'themselves',\n",
              " 'obviously',\n",
              " 'heart',\n",
              " 'parts',\n",
              " 'actress',\n",
              " 'blood',\n",
              " 'flick',\n",
              " 'brilliant',\n",
              " 'directed',\n",
              " 'close',\n",
              " 'ill',\n",
              " 'thinking',\n",
              " 'myself',\n",
              " 'took',\n",
              " 'feeling',\n",
              " 'late',\n",
              " 'decent',\n",
              " 'highly',\n",
              " 'except',\n",
              " 'art',\n",
              " 'stories',\n",
              " 'run',\n",
              " 'fight',\n",
              " 'stuff',\n",
              " 'says',\n",
              " 'heard',\n",
              " 'roles',\n",
              " 'genre',\n",
              " 'happens',\n",
              " 'kid',\n",
              " 'extremely',\n",
              " 'cannot',\n",
              " 'involved',\n",
              " 'leave',\n",
              " 'moment',\n",
              " 'police',\n",
              " 'hand',\n",
              " 'matter',\n",
              " 'killed',\n",
              " 'city',\n",
              " 'wouldnt',\n",
              " 'particularly',\n",
              " 'hell',\n",
              " 'strong',\n",
              " 'living',\n",
              " 'save',\n",
              " 'hour',\n",
              " 'etc',\n",
              " 'lack',\n",
              " 'violence',\n",
              " 'happened',\n",
              " 'age',\n",
              " 'obvious',\n",
              " 'including',\n",
              " 'type',\n",
              " 'voice',\n",
              " 'chance',\n",
              " 'wonder',\n",
              " 'told',\n",
              " 'number',\n",
              " 'shown',\n",
              " 'james',\n",
              " 'simple',\n",
              " 'complete',\n",
              " 'murder',\n",
              " 'coming',\n",
              " 'none',\n",
              " 'itbr',\n",
              " 'score',\n",
              " 'across',\n",
              " 'exactly',\n",
              " 'slow',\n",
              " 'experience',\n",
              " 'daughter',\n",
              " 'god',\n",
              " 'looked',\n",
              " 'anyway',\n",
              " 'attempt',\n",
              " 'ago',\n",
              " 'serious',\n",
              " 'ok',\n",
              " 'whose',\n",
              " 'david',\n",
              " 'please',\n",
              " 'annoying',\n",
              " 'group',\n",
              " 'brother',\n",
              " 'usually',\n",
              " 'running',\n",
              " 'taken',\n",
              " 'sad',\n",
              " 'song',\n",
              " 'opening',\n",
              " 'lets',\n",
              " 'yourself',\n",
              " 'mostly',\n",
              " 'known',\n",
              " 'happen',\n",
              " 'cinematography',\n",
              " 'ends',\n",
              " 'released',\n",
              " 'english',\n",
              " 'finds',\n",
              " 'interest',\n",
              " 'alone',\n",
              " 'wish',\n",
              " 'huge',\n",
              " 'hours',\n",
              " 'scary',\n",
              " 'somewhat',\n",
              " 'hilarious',\n",
              " 'body',\n",
              " 'career',\n",
              " 'started',\n",
              " 'relationship',\n",
              " 'hit',\n",
              " 'stop',\n",
              " 'usual',\n",
              " 'jokes',\n",
              " 'ridiculous',\n",
              " 'cut',\n",
              " 'crap',\n",
              " 'change',\n",
              " 'seriously',\n",
              " 'robert',\n",
              " 'cool',\n",
              " 'possible',\n",
              " 'saying',\n",
              " 'happy',\n",
              " 'opinion',\n",
              " 'musical',\n",
              " 'episodes',\n",
              " '5',\n",
              " 'novel',\n",
              " 'major',\n",
              " 'strange',\n",
              " 'order',\n",
              " 'husband',\n",
              " 'gore',\n",
              " 'directors',\n",
              " 'shots',\n",
              " 'knows',\n",
              " 'call',\n",
              " 'talking',\n",
              " 'knew',\n",
              " 'today',\n",
              " 'songs',\n",
              " 'documentary',\n",
              " 'female',\n",
              " 'rating',\n",
              " 'power',\n",
              " 'events',\n",
              " 'hero',\n",
              " 'clearly',\n",
              " 'reality',\n",
              " 'basically',\n",
              " '4',\n",
              " 'due',\n",
              " 'taking',\n",
              " 'easily',\n",
              " 'talent',\n",
              " 'arent',\n",
              " 'supporting',\n",
              " 'turned',\n",
              " 'level',\n",
              " 'british',\n",
              " 'sequence',\n",
              " 'apparently',\n",
              " 'important',\n",
              " 'words',\n",
              " 'bring',\n",
              " 'television',\n",
              " 'tells',\n",
              " 'room',\n",
              " 'local',\n",
              " 'view',\n",
              " 'king',\n",
              " 'falls',\n",
              " 'modern',\n",
              " 'word',\n",
              " 'whats',\n",
              " 'attention',\n",
              " 'george',\n",
              " 'single',\n",
              " 'sets',\n",
              " 'cheap',\n",
              " 'animation',\n",
              " 'problems',\n",
              " 'future',\n",
              " 'silly',\n",
              " 'miss',\n",
              " 'four',\n",
              " 'entertainment',\n",
              " 'whether',\n",
              " 'beyond',\n",
              " 'light',\n",
              " 'moviebr',\n",
              " 'predictable',\n",
              " 'jack',\n",
              " 'country',\n",
              " 'filmbr',\n",
              " 'enjoyable',\n",
              " 'paul',\n",
              " 'comic',\n",
              " 'feels',\n",
              " 'talk',\n",
              " 'romantic',\n",
              " 'giving',\n",
              " 'storyline',\n",
              " 'similar',\n",
              " 'earth',\n",
              " 'disappointed',\n",
              " 'mention',\n",
              " 'viewers',\n",
              " 'actual',\n",
              " 'richard',\n",
              " 'nearly',\n",
              " 'upon',\n",
              " 'theater',\n",
              " 'within',\n",
              " 'appears',\n",
              " 'five',\n",
              " 'lady',\n",
              " 'moving',\n",
              " 'havent',\n",
              " 'mystery',\n",
              " 'review',\n",
              " 'easy',\n",
              " 'needs',\n",
              " 'lots',\n",
              " 'add',\n",
              " 'points',\n",
              " 'named',\n",
              " 'certain',\n",
              " 'writer',\n",
              " 'theme',\n",
              " 'showing',\n",
              " 'sequel',\n",
              " 'rock',\n",
              " 'fantastic',\n",
              " 'famous',\n",
              " 'begins',\n",
              " 'elements',\n",
              " 'ten',\n",
              " 'among',\n",
              " 'typical',\n",
              " 'above',\n",
              " 'ways',\n",
              " 'tried',\n",
              " 'means',\n",
              " 'comments',\n",
              " 'stay',\n",
              " 'herself',\n",
              " 'surprised',\n",
              " 'middle',\n",
              " 'message',\n",
              " 'hate',\n",
              " 'clear',\n",
              " 'using',\n",
              " 'bunch',\n",
              " 'near',\n",
              " 'thriller',\n",
              " 'french',\n",
              " 'form',\n",
              " 'dull',\n",
              " 'release',\n",
              " 'effort',\n",
              " 'somehow',\n",
              " 'york',\n",
              " 'working',\n",
              " 'tale',\n",
              " 'straight',\n",
              " 'avoid',\n",
              " 'viewing',\n",
              " 'red',\n",
              " 'brought',\n",
              " 'doubt',\n",
              " 'peter',\n",
              " 'editing',\n",
              " 'gone',\n",
              " 'dialog',\n",
              " 'tom',\n",
              " 'soundtrack',\n",
              " 'sister',\n",
              " 'team',\n",
              " 'class',\n",
              " 'season',\n",
              " 'kept',\n",
              " 'greatest',\n",
              " 'feature',\n",
              " 'eye',\n",
              " 'parents',\n",
              " 'particular',\n",
              " 'eventually',\n",
              " 'sorry',\n",
              " 'figure',\n",
              " 'fast',\n",
              " 'de',\n",
              " 'buy',\n",
              " 'leads',\n",
              " 'check',\n",
              " 'japanese',\n",
              " 'filmed',\n",
              " 'general',\n",
              " 'expected',\n",
              " 'space',\n",
              " 'weak',\n",
              " 'oscar',\n",
              " 'fall',\n",
              " 'indeed',\n",
              " 'imagine',\n",
              " 'decided',\n",
              " 'learn',\n",
              " 'material',\n",
              " 'difficult',\n",
              " 'hear',\n",
              " 'period',\n",
              " 'youve',\n",
              " 'sit',\n",
              " 'realistic',\n",
              " 'forget',\n",
              " 'became',\n",
              " 'sequences',\n",
              " 'premise',\n",
              " 'follow',\n",
              " 'reviews',\n",
              " 'poorly',\n",
              " 'move',\n",
              " 'lee',\n",
              " 'lame',\n",
              " 'dance',\n",
              " 'zombie',\n",
              " 'atmosphere',\n",
              " 'whos',\n",
              " 'wait',\n",
              " 'sexual',\n",
              " 'rent',\n",
              " 'deal',\n",
              " 'question',\n",
              " 'suspense',\n",
              " 'surprise',\n",
              " 'stand',\n",
              " 'possibly',\n",
              " '80s',\n",
              " 'whatever',\n",
              " 'stage',\n",
              " 'memorable',\n",
              " 'average',\n",
              " 'writers',\n",
              " 'begin',\n",
              " 'romance',\n",
              " 'die',\n",
              " 'nature',\n",
              " 'third',\n",
              " 'nor',\n",
              " 'believable',\n",
              " 'subject',\n",
              " 'crime',\n",
              " 'write',\n",
              " 'meets',\n",
              " 'killing',\n",
              " 'screenplay',\n",
              " 'okay',\n",
              " 'needed',\n",
              " 'disney',\n",
              " 'filmmakers',\n",
              " 'boys',\n",
              " 'note',\n",
              " 'forced',\n",
              " 'meet',\n",
              " 'situation',\n",
              " 'interested',\n",
              " 'emotional',\n",
              " 'weird',\n",
              " 'unless',\n",
              " 'superb',\n",
              " 'street',\n",
              " 'society',\n",
              " 'otherwise',\n",
              " 'leaves',\n",
              " 'dramatic',\n",
              " 'baby',\n",
              " 'realize',\n",
              " 'towards',\n",
              " 'total',\n",
              " 'reading',\n",
              " 'result',\n",
              " 'previous',\n",
              " 'hot',\n",
              " 'dr',\n",
              " 'directing',\n",
              " 'earlier',\n",
              " 'footage',\n",
              " 'features',\n",
              " 'sounds',\n",
              " 'personal',\n",
              " 'beauty',\n",
              " 'shame',\n",
              " 'older',\n",
              " 'laughs',\n",
              " 'create',\n",
              " 'joe',\n",
              " 'worked',\n",
              " 'credits',\n",
              " 'creepy',\n",
              " 'admit',\n",
              " 'truth',\n",
              " 'quickly',\n",
              " 'keeps',\n",
              " 'brings',\n",
              " 'whom',\n",
              " 'deep',\n",
              " 'brothers',\n",
              " 'development',\n",
              " 'effect',\n",
              " 'imdb',\n",
              " 'minute',\n",
              " 'mark',\n",
              " 'battle',\n",
              " 'perfectly',\n",
              " 'match',\n",
              " 'incredibly',\n",
              " 'america',\n",
              " 'return',\n",
              " 'plenty',\n",
              " 'ask',\n",
              " 'leading',\n",
              " 'crazy',\n",
              " 'appear',\n",
              " 'various',\n",
              " 'male',\n",
              " 'hands',\n",
              " '70s',\n",
              " 'comment',\n",
              " 'cheesy',\n",
              " 'unique',\n",
              " 'badly',\n",
              " 'b',\n",
              " 'scifi',\n",
              " 'present',\n",
              " 'open',\n",
              " 'free',\n",
              " 'plus',\n",
              " 'meant',\n",
              " 'ideas',\n",
              " 'remake',\n",
              " 'setting',\n",
              " 'apart',\n",
              " '20',\n",
              " 'masterpiece',\n",
              " 'portrayed',\n",
              " 'casting',\n",
              " 'inside',\n",
              " 'hardly',\n",
              " 'dog',\n",
              " 'business',\n",
              " 'air',\n",
              " 'potential',\n",
              " 'manages',\n",
              " 'fighting',\n",
              " 'front',\n",
              " 'dumb',\n",
              " 'mess',\n",
              " 'fairly',\n",
              " 'fails',\n",
              " 'gay',\n",
              " 'background',\n",
              " 'expecting',\n",
              " 'nudity',\n",
              " 'missing',\n",
              " 'la',\n",
              " 'christmas',\n",
              " 'twist',\n",
              " 'sadly',\n",
              " 'recently',\n",
              " 'monster',\n",
              " 'dream',\n",
              " 'bill',\n",
              " 'secret',\n",
              " 'fire',\n",
              " 'attempts',\n",
              " 'outside',\n",
              " 'joke',\n",
              " 'talented',\n",
              " 'success',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_1gram_train_ds = train_ds.map(lambda x,y : (text_vectorization(x),y), num_parallel_calls=4 )\n",
        "binary_1gram_val_ds = val_ds.map(lambda x,y : (text_vectorization(x),y), num_parallel_calls=4 )\n",
        "binary_1gram_test_ds = test_ds.map(lambda x,y : (text_vectorization(x),y), num_parallel_calls=4 )"
      ],
      "metadata": {
        "id": "NP9oF0G6nQ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs,targets in binary_1gram_train_ds:\n",
        "  print(inputs.shape)\n",
        "  print(targets.shape)\n",
        "  print(inputs[0].dtype)\n",
        "  print(targets[0].shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buZZpoernz4q",
        "outputId": "3ea7c9bb-8388-4488-a56f-0da4264435c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 20000)\n",
            "(32,)\n",
            "<dtype: 'float32'>\n",
            "()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reusable model building utility"
      ],
      "metadata": {
        "id": "a7kpSkWioobB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "dyZcQe_hoIMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(max_tokens = 20000,hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\") (inputs)\n",
        "  x = layers.Dropout(0.5) (x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\") (x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "U5YBRgNAozCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()"
      ],
      "metadata": {
        "id": "0r4_3uyYozdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjJDbRQpzMJ",
        "outputId": "417a509c-eae7-4dfc-9db3-c03f39f00df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "HtR7uNeZqton"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(binary_1gram_train_ds.cache(),validation_data = binary_1gram_val_ds.cache(), epochs = 10, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhgWqQ4pqxur",
        "outputId": "be554b11-0d9b-4a9a-f4f6-a3a4b691d2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 9ms/step - loss: 0.4106 - accuracy: 0.8283 - val_loss: 0.2940 - val_accuracy: 0.8824\n",
            "Epoch 2/10\n",
            " 46/625 [=>............................] - ETA: 2s - loss: 0.2947 - accuracy: 0.8886"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2773 - accuracy: 0.8975 - val_loss: 0.2891 - val_accuracy: 0.8852\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2393 - accuracy: 0.9166 - val_loss: 0.3003 - val_accuracy: 0.8870\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2231 - accuracy: 0.9255 - val_loss: 0.3164 - val_accuracy: 0.8866\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2103 - accuracy: 0.9294 - val_loss: 0.3308 - val_accuracy: 0.8848\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2057 - accuracy: 0.9303 - val_loss: 0.3493 - val_accuracy: 0.8862\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2039 - accuracy: 0.9337 - val_loss: 0.3538 - val_accuracy: 0.8862\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2027 - accuracy: 0.9359 - val_loss: 0.3660 - val_accuracy: 0.8838\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1973 - accuracy: 0.9391 - val_loss: 0.3763 - val_accuracy: 0.8862\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1993 - accuracy: 0.9382 - val_loss: 0.3884 - val_accuracy: 0.8798\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7929cc309b40>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary_1gram_train_ds is a map from train_ds.so everytime an epoch is called, the strings from train_ds are taken in batches, text vectorization is done on it [map function], then fed to the weights which are loaded in the gpu. this preprocessing if done in every epoch is redundant and happens on the cpu making the gpu wait.instead we call binary_1gram_train_ds.cache() to store the preprocessed data in cache memory during first epoch [works for smaller sized data]"
      ],
      "metadata": {
        "id": "5-cYd8e4rFag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "an accuracy of 88.7 percent is achieved with this simple model itself beating the baseline accuracy of 50 percentage , which is cool.the objective is to increase the accuracy to as much as possible."
      ],
      "metadata": {
        "id": "4jfLRJucrr87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"binary_1gram.h5\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b2cdfGJq-m0",
        "outputId": "9b77a5fb-9c5e-4e4d-da36-ea97bd441ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2876 - accuracy: 0.8864\n",
            "Test acc: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ds object is an iterator object of fixed bytes which loops through data and returns batch size data to the caller.\n",
        "# so the entire data is not stored as arrays [creating copies], they are processed on the fly in the cpus during training / inference\n",
        "#hence the need to cache it during training\n",
        "#import sys\n",
        "#sys.getsizeof(binary_1gram_train_ds)\n"
      ],
      "metadata": {
        "id": "xm23mRihsEAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram encoding : even simple pair of words can carry context like 'stand up' , sit down etc which often come in pairs."
      ],
      "metadata": {
        "id": "SummtlnS0ZTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(ngrams=2,max_tokens= 20000,output_mode=\"multi_hot\")"
      ],
      "metadata": {
        "id": "nNTkuL90yapn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "ZspF_J2g1Dn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9J2TdLK2gDv",
        "outputId": "98709028-9a47-4f15-e47e-ca0d7b4ae50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " 'the',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'it',\n",
              " 'i',\n",
              " 'this',\n",
              " 'that',\n",
              " 'br',\n",
              " 'was',\n",
              " 'as',\n",
              " 'with',\n",
              " 'for',\n",
              " 'but',\n",
              " 'movie',\n",
              " 'of the']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_2gram_train_ds = train_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "xoMM_R302kCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2NNFZFx2ydc",
        "outputId": "a2801365-1bfc-4fd9-eb68-7430d7f352df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds.cache(), epochs=10,\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdf-SfNN216P",
        "outputId": "f738f797-199a-4a91-c662-222bcf1f5d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 9ms/step - loss: 0.3854 - accuracy: 0.8397 - val_loss: 0.2827 - val_accuracy: 0.8902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2428 - accuracy: 0.9130 - val_loss: 0.2825 - val_accuracy: 0.8948\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2028 - accuracy: 0.9323 - val_loss: 0.3062 - val_accuracy: 0.8916\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1813 - accuracy: 0.9431 - val_loss: 0.3161 - val_accuracy: 0.8960\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1742 - accuracy: 0.9484 - val_loss: 0.3513 - val_accuracy: 0.8890\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1617 - accuracy: 0.9509 - val_loss: 0.3508 - val_accuracy: 0.8944\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1629 - accuracy: 0.9521 - val_loss: 0.3577 - val_accuracy: 0.8934\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1538 - accuracy: 0.9560 - val_loss: 0.3656 - val_accuracy: 0.8962\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1477 - accuracy: 0.9584 - val_loss: 0.3842 - val_accuracy: 0.8954\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1527 - accuracy: 0.9566 - val_loss: 0.3870 - val_accuracy: 0.8910\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea7503c90f0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"binary_2gram.h5\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSGQEN2RCo1z",
        "outputId": "84f3ce0b-6335-46d8-be51-c90e91a2049c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2792 - accuracy: 0.8952\n",
            "Test acc: 0.895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "an improvement of 1 percent accuracy with bigram model!"
      ],
      "metadata": {
        "id": "7Ht4VRtvC2bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let us try bigram encoding with tf-idf : term frequency - inverse document frequency\n",
        "the sparsity property is embedded in our bigram multi-hot encoded representation\n",
        "that is only those terms which appear in the document frequently are given 1 and others are zeros -> this is a useful property which reduces computational load and also prevents overfitting.\n",
        "one way of introducing normalization is subtracting the mean of the term score appearing across documents and dividing the variance -> feature normalization, but this would disturb normalization.hence we need to divide by a quantity alone\n",
        "here is where tf-idf comes in., there are some terms which appear many times in the document like a, the, as, which don't contribute much towards sentiment classification.\n",
        "hence , we can find document frequency -> number of times the word appears across the documents and divide by the term frequency, so that unique words which matter to this document are picked up.\n",
        "\n",
        "def tf_idf(term, document, dataset):\n",
        "    term_freq = document.count(term)\n",
        "    doc_freq = math.log(sum(doc.count(term) for doc in dataset)+1)\n",
        "    return term_freq/doc_freq\n",
        "    "
      ],
      "metadata": {
        "id": "hKLsiQu6C8dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the tf-idf is build as a functionality in keras text_vectorization module."
      ],
      "metadata": {
        "id": "qkdlBqiHF9fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(ngrams=2, max_tokens = 20000, output_mode = \"tf-idf\")"
      ],
      "metadata": {
        "id": "FBz4T2jvC5YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "oaHYcutuGO8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create tf_idf ds for train, val and test as a map to train_ds, val_ds, test_ds with cpu threading num_parallel calls=4\n",
        "\n",
        "tf_idf_2gram_train_ds = train_ds.map(lambda x,y : (text_vectorization(x),y), num_parallel_calls=4)\n",
        "tf_idf_2gram_val_ds = val_ds.map(lambda x,y : (text_vectorization(x),y), num_parallel_calls=4)\n",
        "tf_idf_2gram_test_ds = test_ds.map(lambda x,y : (text_vectorization(x),y), num_parallel_calls=4)\n"
      ],
      "metadata": {
        "id": "6xL7-zlZGVic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()"
      ],
      "metadata": {
        "id": "KaIIyUMoGt-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get summary of the model\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFgEzTGpGvrO",
        "outputId": "e20a2873-1dd8-4f91-bb4a-e942e7cd198b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a callback to store the model as 'tf_idf_2gram.h5' with save_best_only as True\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tf_idf_2gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n"
      ],
      "metadata": {
        "id": "HPW4Aj7XIlnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf_idf_2gram_train_ds.cache(),\n",
        "validation_data=tf_idf_2gram_val_ds.cache(), epochs=10,\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsQOQ5AQIycm",
        "outputId": "863a3a4b-c7bf-4b33-d6d1-17bc79126f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 7s 10ms/step - loss: 0.4889 - accuracy: 0.7930 - val_loss: 0.3317 - val_accuracy: 0.8486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3196 - accuracy: 0.8737 - val_loss: 0.3043 - val_accuracy: 0.8774\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.2730 - accuracy: 0.8910 - val_loss: 0.3330 - val_accuracy: 0.8884\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2491 - accuracy: 0.8972 - val_loss: 0.3441 - val_accuracy: 0.8736\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2432 - accuracy: 0.9018 - val_loss: 0.3419 - val_accuracy: 0.8754\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2299 - accuracy: 0.9064 - val_loss: 0.3516 - val_accuracy: 0.8764\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2194 - accuracy: 0.9075 - val_loss: 0.3787 - val_accuracy: 0.8698\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2111 - accuracy: 0.9090 - val_loss: 0.3910 - val_accuracy: 0.8666\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2077 - accuracy: 0.9111 - val_loss: 0.3819 - val_accuracy: 0.8752\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2030 - accuracy: 0.9137 - val_loss: 0.3895 - val_accuracy: 0.8590\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea6dbb1e620>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: based on model.fit() results above, put your inference and analysis as a markdown text\n",
        "\n",
        "The model.fit() results show that the model trained on the tf-idf_2gram_train_ds dataset achieved a test accuracy of 89.9%. This is an improvement over the test accuracy of 88.7% achieved by the model trained on the binary_1gram_train_ds dataset and the test accuracy of 89.2% achieved by the model trained on the binary_2gram_train_ds dataset.\n",
        "\n",
        "This suggests that using tf-idf weighting can be beneficial for sentiment classification tasks, as it can help to identify and emphasize the most important words and phrases in the text.\n",
        "\n",
        "Further improvements to the model could be explored by experimenting with different hyperparameters, such as the number of epochs, the learning rate, and the number of hidden units in the neural network. Additionally, other types of word embeddings, such as word2vec or GloVe, could be used to represent the words in the text."
      ],
      "metadata": {
        "id": "Ibrt4NVjI-j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"tf_idf_2gram.h5\")\n",
        "print(f\"Test acc: {model.evaluate(tf_idf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpHiuonbJZ_O",
        "outputId": "25800431-2f83-41e7-bb40-c54f6bb72191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2962 - accuracy: 0.8803\n",
            "Test acc: 0.880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is important to be careful with gen AI generated code and commentaries -> though it is very useful for asistance in finding APIs, hyperparameters, if it is not sure of something, it confidently puts a wrong number, for example it confidently claims that an accuracy of 89.9 percent was obtained with tf-idf ,whereas 88.03 is the actual test accuracy and for this tf-idf does not produce a better accuracy but usualy it produces better accuracy for say large datasets.\n"
      ],
      "metadata": {
        "id": "Dp3Bbru0Jj5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential models : Treat the text as a sequence of integer indices which will be mapped to a corresponding vector embedding."
      ],
      "metadata": {
        "id": "9PTZeby8w-Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "max_length = 600 # truncate length of reviews to 600\n",
        "max_tokens = 20000 # max tokens in vocab\n",
        "text_vectorization = layers.TextVectorization(max_tokens = max_tokens, output_mode = \"int\", output_sequence_length=max_length)\n",
        "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n"
      ],
      "metadata": {
        "id": "yyTA4jnAUe9y"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary() [:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p0z7dhayFVb",
        "outputId": "0f3555bb-6889-4c54-ddaa-390c2dc3d995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'it',\n",
              " 'i',\n",
              " 'this',\n",
              " 'that',\n",
              " 'br',\n",
              " 'was',\n",
              " 'as',\n",
              " 'for',\n",
              " 'with',\n",
              " 'movie',\n",
              " 'but']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int_train_ds = train_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "XyqoDWogx4Vb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "one-hot encode the integer sequences :\n",
        "embedding size becomes (600,20000)"
      ],
      "metadata": {
        "id": "F8ximm9aytiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in int_train_ds:\n",
        "  print(j.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ult4WntPy1tZ",
        "outputId": "2c31feee-18be-473f-cdc6-72df8811d789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,),dtype='int64')\n",
        "embedded = tf.one_hot(inputs,depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32)) (embedded)\n",
        "x = layers.Dropout(0.5) (x)\n",
        "output = layers.Dense(1, activation='sigmoid') (x)\n",
        "model = keras.Model(inputs = inputs, outputs= output)\n"
      ],
      "metadata": {
        "id": "q0a3DwJuzDWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: compile the model with rmsprop optimiser, metrics as accuracy and loss function as binary cross entropy\n",
        "\n",
        "model.compile(optimizer='rmsprop', metrics=['accuracy'], loss='binary_crossentropy')\n"
      ],
      "metadata": {
        "id": "OrHZCK8SzzWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4-jZY520NZJ",
        "outputId": "ea72c481-9f70-4e7d-c703-aad80619f8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot_2 (TFOpLambda)   (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                5128448   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5128513 (19.56 MB)\n",
            "Trainable params: 5128513 (19.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.h5\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "1t7HO8qY0Pi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the training takes a lot of time because each sequence is 600 words and embedding size is 200000. whereas in bigram model, it was a bunch of bigrams stored in dictionary and used in a de"
      ],
      "metadata": {
        "id": "e9sFAfpt0shl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7NRiXUs0VTe",
        "outputId": "b2488afd-4fe9-4cb5-ab9f-bdb135b9ff54"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 164s 252ms/step - loss: 0.5567 - accuracy: 0.7245 - val_loss: 0.4347 - val_accuracy: 0.8054\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 165s 264ms/step - loss: 0.3635 - accuracy: 0.8666 - val_loss: 0.3449 - val_accuracy: 0.8602\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 164s 262ms/step - loss: 0.2936 - accuracy: 0.8959 - val_loss: 0.3348 - val_accuracy: 0.8640\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 166s 266ms/step - loss: 0.2460 - accuracy: 0.9147 - val_loss: 0.3061 - val_accuracy: 0.8910\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 165s 264ms/step - loss: 0.2166 - accuracy: 0.9261 - val_loss: 0.3742 - val_accuracy: 0.8832\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 166s 266ms/step - loss: 0.2004 - accuracy: 0.9354 - val_loss: 0.2869 - val_accuracy: 0.8856\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 165s 263ms/step - loss: 0.1694 - accuracy: 0.9443 - val_loss: 0.3334 - val_accuracy: 0.8752\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 166s 266ms/step - loss: 0.1501 - accuracy: 0.9530 - val_loss: 0.3449 - val_accuracy: 0.8826\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 165s 264ms/step - loss: 0.1302 - accuracy: 0.9593 - val_loss: 0.4264 - val_accuracy: 0.8812\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 166s 265ms/step - loss: 0.1169 - accuracy: 0.9628 - val_loss: 0.3597 - val_accuracy: 0.8730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c8bcbf62830>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "87.3 percent accuracy is observed, which is worse than the fast bigram model -> clearly the model is struggling to process (600, 20000) for each review"
      ],
      "metadata": {
        "id": "G3WK-OgX7T-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand word embeddings : One-hot encoding is a sort of feature engineering -> we are injecting an assumption about the word structure that they are independent of one another , one hot encoded vectors are all orthogonal to one another. Reality is very different -> the words are all related to one another either semantically similar or dissimilar or sometimes antagonastic. word embeddings try to capture semantic similarity of words in terms of geometric patterns.\n",
        "one-hot encoded vector : sparse -> boolean -> high dimensional\n",
        "embedding vector : dense -> float -> low dimensional\n",
        "word-embedding are also learned instead of hardcoded.\n",
        "word embeddings are as real asit gets if trained properly -> with female vector added to king, we go to embedding of queen etc\n",
        "\n",
        "Word embeddings could be task specific or pre-trained from another task."
      ],
      "metadata": {
        "id": "lPuOuv2e7jqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim = max_tokens, output_dim = 256)"
      ],
      "metadata": {
        "id": "KpZmCFRQ7fXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer is a dictionary lookup which maps integer indices to vector"
      ],
      "metadata": {
        "id": "uh2tcyQMDkho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype='int64')\n",
        "embedded = layers.Embedding(input_dim = max_tokens, output_dim = 256) (inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32)) (embedded)\n",
        "x= layers.Dropout(0.5) (x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\") (x)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZuezCWeDDglG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1QMFZV-Y1xM",
        "outputId": "5cf4c23a-3d00-4182-add1-4657cf7a0846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', metrics = ['accuracy'], loss = 'binary_crossentropy')"
      ],
      "metadata": {
        "id": "N6oaw54PY_ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: creata a kerbs callback object with model checkpoint saved to 'bidirectional_embedding.h5' file with best model saved\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"bidirectional_embedding.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n"
      ],
      "metadata": {
        "id": "chJamnI8Zbnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXWz79C4ZNWB",
        "outputId": "bc708ef6-cbeb-41e5-8664-ff2e9cfd37ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 64s 91ms/step - loss: 0.5340 - accuracy: 0.7369 - val_loss: 0.4561 - val_accuracy: 0.8244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "625/625 [==============================] - 41s 65ms/step - loss: 0.3576 - accuracy: 0.8659 - val_loss: 0.4805 - val_accuracy: 0.7928\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.2846 - accuracy: 0.8959 - val_loss: 0.3269 - val_accuracy: 0.8838\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 0.2417 - accuracy: 0.9173 - val_loss: 0.3490 - val_accuracy: 0.8728\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 0.2064 - accuracy: 0.9298 - val_loss: 0.3447 - val_accuracy: 0.8702\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.1767 - accuracy: 0.9427 - val_loss: 0.3900 - val_accuracy: 0.8674\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.1536 - accuracy: 0.9501 - val_loss: 0.4924 - val_accuracy: 0.8556\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.1274 - accuracy: 0.9589 - val_loss: 0.4343 - val_accuracy: 0.8708\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.1094 - accuracy: 0.9665 - val_loss: 0.4468 - val_accuracy: 0.8698\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.0975 - accuracy: 0.9707 - val_loss: 0.4490 - val_accuracy: 0.8646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bcd76e0f9a0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"bidirectional_embedding.h5\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5CBVBJmZviP",
        "outputId": "1a63fc43-5bdb-43b9-a004-ebfe0957de35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 17s 21ms/step - loss: 0.3403 - accuracy: 0.8754\n",
            "Test acc: 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the accuracy is still not that great it is just above 87.5 percent but the model with word embedding train much faster. still not better tha bigram model.\n",
        "one reason could be the truncation of review to 600 words. maybe some information is being lost. also , if the number of words is less than 600, they are padded with 0, so an bidirectional lstm which processes data in both natural order and reverse order, we see that some tokens are zeros and the original meaning which was learnt from the initial words are getting lost with the meaningless inputs. we need to mask the zeros to feed the rnn with meaningful words only."
      ],
      "metadata": {
        "id": "zdWg92Wdbm7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in embedding api, mask_Zero is available, which masks the places which have zero values, skipping those for RNN computations\n",
        "Keras will pass the masks as metadata to every layer which process the data.in case sequnce of data is passed to the loss function during training, the masked portions will be skiiped to compute loss"
      ],
      "metadata": {
        "id": "YdfAN_Q5i0P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "loss=\"binary_crossentropy\",\n",
        "metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03RYitu9bh-z",
        "outputId": "f8a744aa-5e14-43aa-f87e-1454c6e6dd7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: creata a cerasa callback function to save model checkpoint with name \"lstm_embedding_masked.h5\" saving the best model.\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"lstm_embedding_masked.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynpfubWMj9pg",
        "outputId": "d0553ea7-3022-42bc-8492-f98476190b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 67s 94ms/step - loss: 0.4431 - accuracy: 0.7904 - val_loss: 0.3250 - val_accuracy: 0.8660\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.2816 - accuracy: 0.8899 - val_loss: 0.2935 - val_accuracy: 0.8726\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 40s 65ms/step - loss: 0.2210 - accuracy: 0.9165 - val_loss: 0.2970 - val_accuracy: 0.8812\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1706 - accuracy: 0.9386 - val_loss: 0.3487 - val_accuracy: 0.8822\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.1378 - accuracy: 0.9500 - val_loss: 0.3346 - val_accuracy: 0.8864\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1043 - accuracy: 0.9647 - val_loss: 0.4329 - val_accuracy: 0.8782\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 30s 49ms/step - loss: 0.0808 - accuracy: 0.9732 - val_loss: 0.3842 - val_accuracy: 0.8800\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.0633 - accuracy: 0.9785 - val_loss: 0.3954 - val_accuracy: 0.8742\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.0475 - accuracy: 0.9847 - val_loss: 0.5676 - val_accuracy: 0.8652\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 33s 54ms/step - loss: 0.0384 - accuracy: 0.9878 - val_loss: 0.4520 - val_accuracy: 0.8718\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bcde0ec8880>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "just like how for vision applications, pretrained convnets were used with finetuning applied on problem specific dataset, pre-trained embeddings trained on vast corpus of data can be used as embeddings whcih capture generic language patterns and semantic features. Such pre-trained embeddings may have been done using word-statistic analysis - like words occur togetehr across documetns etc or a neural network dedicated to it.\n",
        "Word2Vec is one of the famous word-embedding algorithm. Another famous word embedding scheme is Global vectors for word representation - Glove\n"
      ],
      "metadata": {
        "id": "aBLq0lGvsnkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove - 100 dimensional embedding for 400000 words"
      ],
      "metadata": {
        "id": "RwQsNi_EujqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs6lmTHhsbAw",
        "outputId": "b3c3bed3-4e06-49d8-8b5c-abe8718ddf80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 03:12:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-04-20 03:12:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-20 03:12:50--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: glove.6B.zip\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-04-20 03:15:29 (5.19 MB/s) - glove.6B.zip saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "KpMi7i4LurVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the glove text file into a dictionary"
      ],
      "metadata": {
        "id": "BKqR8cacj6T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\""
      ],
      "metadata": {
        "id": "1EffJEKbfiUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, dtype='float', sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "print(f\"found {len(embeddings_index)} word vectors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kGtmF-PkCTX",
        "outputId": "179d47a4-ded4-4b80-bb17-6ae3adb8ddef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "print(len(vocabulary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foTpiGFokwLs",
        "outputId": "16d7e8fc-59da-4c47-cec4-a4db0ba59a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "len(word_index\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcIUxn2OlkpJ",
        "outputId": "ae8d5e5f-564d-44ce-9179-afee38405a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2amFgzr9mMOU",
        "outputId": "8cf0ad08-935c-4d19-a0c1-09c0c9011034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'and', 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((max_tokens,embedding_dim))"
      ],
      "metadata": {
        "id": "t7k4idCqmQYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, i in word_index.items():\n",
        "  if i < max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "3YZBpVC8mX0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(max_tokens,embedding_dim,embeddings_initializer=keras.initializers.constant(embedding_matrix), trainable=False, mask_zero=True)"
      ],
      "metadata": {
        "id": "nLBfnRC7nX41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this pretrained embedding shoudl not be distrubed during training, so we use trainable = false for embedding layer and load this constant embedding matrix as initializer"
      ],
      "metadata": {
        "id": "Ej4pPHodnZtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a keras model which symbolic input data , has a LSTM with 32 hidden units in bidirectional mode, has a dropout of 0.5 as next layer and the final layer as dense layer with 1 neutron and sigmoid activation\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype='int64')\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32)) (embedded)\n",
        "x = layers.Dropout(0.5) (x)\n",
        "output = layers.Dense(1, activation='sigmoid') (x)\n",
        "model = keras.Model(inputs = inputs, outputs= output)\n",
        "model.compile(optimizer='rmsprop', metrics=['accuracy'], loss='binary_crossentropy')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uWSgnBOn9LZ",
        "outputId": "c695023f-f3f4-4e6b-e105-0f3a13c6eb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                34048     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2034113 (7.76 MB)\n",
            "Trainable params: 34113 (133.25 KB)\n",
            "Non-trainable params: 2000000 (7.63 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: creat a keras callback to store model checkpoint and save the best model to pretrained_embed_gru.h5\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"pretrained_embed_gru.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n"
      ],
      "metadata": {
        "id": "rggYvfq2oXbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLJjncAUojRq",
        "outputId": "7dda91b2-197a-4458-90a6-a03b95773a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.5783 - accuracy: 0.6910"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r625/625 [==============================] - 56s 68ms/step - loss: 0.5782 - accuracy: 0.6909 - val_loss: 0.4778 - val_accuracy: 0.7676\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 39s 63ms/step - loss: 0.4480 - accuracy: 0.7955 - val_loss: 0.4031 - val_accuracy: 0.8194\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 56s 89ms/step - loss: 0.3995 - accuracy: 0.8269 - val_loss: 0.3995 - val_accuracy: 0.8228\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3675 - accuracy: 0.8435 - val_loss: 0.3843 - val_accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.3466 - accuracy: 0.8544 - val_loss: 0.3429 - val_accuracy: 0.8462\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 47s 76ms/step - loss: 0.3261 - accuracy: 0.8632 - val_loss: 0.3530 - val_accuracy: 0.8450\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 41s 65ms/step - loss: 0.3104 - accuracy: 0.8719 - val_loss: 0.3246 - val_accuracy: 0.8650\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.2922 - accuracy: 0.8821 - val_loss: 0.3248 - val_accuracy: 0.8624\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 52s 84ms/step - loss: 0.2789 - accuracy: 0.8875 - val_loss: 0.3169 - val_accuracy: 0.8716\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.2675 - accuracy: 0.8928 - val_loss: 0.3182 - val_accuracy: 0.8682\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fdb08c861d0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there is no difference with using pre-trained embeddings for this use-case. usually for smalelr datasets, pre-trained embedding give a boost in accuracy,but in this case the dataset itself contains enough information"
      ],
      "metadata": {
        "id": "6P3DP0bZqwL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer architecture"
      ],
      "metadata": {
        "id": "Bee8WTtF8-aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(input_sequence):\n",
        "  output = np.zeros(shape=input_sequence.shape)\n",
        "  for i, pivot_vector in enumerate(input_sequence):\n",
        "    scores = np.zeros(shape=(len(input_sequence),))\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      scores[j] = np.dot(pivot_vector, vector.T)\n",
        "    scores /= np.sqrt(input_sequence.shape[1])\n",
        "    scores = softmax(scores)\n",
        "    new_pivot_vector = np.zeros(shape=pivot_vector.shape)\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      new_pivot_vector += vector * scores[j]\n",
        "    output[i] = new_pivot_vector\n",
        "  return output\n",
        "\n"
      ],
      "metadata": {
        "id": "MrLaBbhuouh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in practice, however a vectorized implementation is used and keras has a layer -> multihead attention to do it for us.\n",
        "Transformer architecture made use of every method used to train deep neural networks\n",
        "-> representation of output latent space refactored into independent sub spaces [multihead attention]\n",
        "-> skip connections to prevent vanishing/exploding gradients [residual networks]\n",
        "-> layernorm to prevent internal covariate shift\n",
        "\n",
        "Transformer.encoder - process the source sequence\n",
        "Transformer decoder - use the processed sequence to generate translated version\n",
        "in the original paper used for machine translation application\n",
        "\n",
        "Transformer encoder can be used for text classification since it can ingest a sequence , process it and use it for other tasks well."
      ],
      "metadata": {
        "id": "jlKr6muzGe2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "9pMp1W5EGa7N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U-TikEHKGduI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "       super().__init__(**kwargs)\n",
        "       self.embed_dim = embed_dim\n",
        "       self.dense_dim = dense_dim\n",
        "       self.num_heads = num_heads\n",
        "       self.attention = layers.MultiHeadAttention(\n",
        "       num_heads=num_heads, key_dim=embed_dim)\n",
        "       self.dense_proj = keras.Sequential(\n",
        "    [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "     layers.Dense(embed_dim),])\n",
        "       self.layernorm_1 = layers.LayerNormalization()\n",
        "       self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "      if mask is not None:\n",
        "\n",
        "\n",
        "        mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "\n",
        "           \"embed_dim\": self.embed_dim,\n",
        "           \"num_heads\": self.num_heads,\n",
        "           \"dense_dim\": self.dense_dim,\n",
        "             })\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CNS9yAcewSkn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "storing the config params of a layer in a dict will be useful during saving/loading a model.\n",
        "Example:\n",
        "layer = PositionalEmbedding(sequence_length, input_dim, output_dim)\n",
        "config = layer.get_config()\n",
        "new_layer = PositionalEmbedding.from_config(config)\n",
        "\n",
        "layer's configs are stored as dictionary and can be used by another layer with the config params loaded."
      ],
      "metadata": {
        "id": "oS63laZJAXfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_normalization(batch_of_sequences):\n",
        "    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    return (batch_of_sequences - mean) / variance"
      ],
      "metadata": {
        "id": "PwFcg59N97TQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "layer normalization is done along the last axis . batch_of_sequences -> batch_size, input_sequence, embedding_dim. so, the mean and var are computed for embedding dim vector for each input_sequence word."
      ],
      "metadata": {
        "id": "a54HMc8W99lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the transformer encoder for text classification\n"
      ],
      "metadata": {
        "id": "eXpc61wp0eL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inputs)\n",
        "mask = layers.Embedding(vocab_size, embed_dim, mask_zero=True).compute_mask(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x,mask)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q843wOwDBXUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: compile the model with rmsprop, loss as binarycross entropy and metrics as accuracy\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "0qrLh1AmJP-D"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JCE_nx-Jq7M",
        "outputId": "0e41bcb8-2ea8-40c4-b3ac-22e10b58a8a8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder_7 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_5 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"transformer_encoder.h5\", save_best_only=True)]"
      ],
      "metadata": {
        "id": "Y3q1GhODOfJ1"
      },
      "execution_count": 50,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}