{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["text_file = \"spa-eng/spa.txt\" \n","with open(text_file) as f:\n","        lines = f.read().split(\"\\n\")[:-1]\n","        text_pairs = []\n","        for line in lines:\n","            english, spanish = line.split(\"\\t\") \n","            spanish = \"[start] \" + spanish + \" [end]\" \n","            text_pairs.append((english, spanish))\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import random\n","print(random.choice(text_pairs))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#train validation split\n","num_samples = len(text_pairs)\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15*num_samples)\n","num_train_samples = num_samples - 2 * num_val_samples\n","train_samples = text_pairs[:num_train_samples]\n","val_samples = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_samples = text_pairs[num_train_samples + num_val_samples:]"]},{"cell_type":"markdown","metadata":{},"source":["usually we strip all the punctuation in both english and spanish texts , but we have inserted [start] and [end] special tokens which are special tokens and hence [] should not be stripped. also an additional character in spanish is also there, which needs to be stripped\n","In certain applications, puncuations are also added as tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import re\n","import string\n","\n","strip_chars = string.punctuation + \"Â¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(text):\n","    lowercase = tf.strings.lower(text)\n","    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\",\"\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_size = 15000 # top 15000 tokens in each language\n","sequence_length = 20 # pick top 20 words in sentence\n","\n","source_vectorization = layers.TextVectorization(max_tokens = vocab_size, output_mode = \"int\", output_sequence_length = sequence_length)\n","target_vectorization = layers.TextVectorization(max_tokens = vocab_size, output_mode = \"int\", output_sequence_length = sequence_length+1, standardize = custom_standardization)\n","# if we don't pass standardize, it will do default standardization -> remove punctuations + lowercase\n","# for target, we need to remove [] from character strip and also spanish symbol is to stripped too\n","train_english_texts = [pair[0] for pair in train_samples]\n","train_spanish_texts = [pair[1] for pair in train_samples]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"]},{"cell_type":"raw","metadata":{},"source":["our data structure : inputs, target where input is the encoder input - english sentence, decoder input, spanish sentence, target -> decoder output , the spanish sentence offset by 1 word"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 64\n","def format_dataset(eng,spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\"english\":eng, \"spanish\":spa[:,:-1]},spa[:,1:])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_dataset(pairs):\n","    eng_text, spa_text = zip(*pairs)\n","    eng_text = list(eng_text)\n","    spa_text = list(spa_text)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_text,spa_text))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n","    return dataset.shuffle(2048).prefetch(16).cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_ds = make_dataset(train_samples)\n","val_ds = make_dataset(val_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for input,targets in train_ds.take(1):\n","    print(\"the shape of english text is \", input['english'].shape)\n","    print(\"the shape of spanish text is \", input['spanish'].shape)\n","    print(\"the shape of target text is \", targets.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Dataset is ready. first we ll try out the data on recurrent network based sequence-sequence model"]},{"cell_type":"markdown","metadata":{},"source":["RNNs specially LSTMs and their variants like GRUs were SOTA models for NLP tasks. \n","for this machine translation task , we could use RNNs with sequence=True.\n","The requirement is that input sequence_length = output_sequence length, this can be managed by padding source or target sequence. another disadvantage is that for prediction of token N, you get to look at tokens 0...N-1, but for translation having access to tokens after N of the source sequence can be beneficial."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#a potential model\n","\n","inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n","x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n","x = layers.LSTM(32, return_sequences=True)(x)\n","outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","model = keras.Model(inputs, outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["we see that the sequence length is maintained by the model with every pass through"]},{"cell_type":"markdown","metadata":{},"source":["one solution is that the RNN parses through the source sequence and upon the end of the sequence,it produces a output vector or the internal state vector in the end could be used which has encoded the input sequence. Then this vector can be used as input to the decoder, which will produce the token N+1 given tokens 0...N and the learned vector at the end of encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embed_dim = 256\n","latent_dim = 1024\n","\n","source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\") # the english source sentence goes here\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True) (source)\n","encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\") (x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["past_target = keras.Input(shape=(None,), dtype=\"int64\",name=\"spanish\") # past spanish tokens in decoder\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True) (past_target)\n","decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n","x = decoder_gru(x , initial_state = encoded_source)\n","x  = layers.Dropout(0.5) (x)\n","model2= keras.Model(past_target, x)\n","print(model2.summary())\n","\n","target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x) \n","seq2seq_rnn = keras.Model([source, past_target], target_next_step)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["seq2seq_rnn.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["seq2seq_rnn.fit(train_ds, validation_data = val_ds, epochs=15)"]},{"cell_type":"markdown","metadata":{},"source":["translation of an english sentence given a seed token [start] -> inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","spanish_vocab = target_vectorization.get_vocabulary()\n","spanish_index_lookup = dict(zip(range(len(spanish_vocab)),spanish_vocab))\n","max_decoded_sequence_length = 20\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    target_sentence = \"[start]\"\n","    for i in range(max_decoded_sequence_length):\n","        tokenized_target_sentence = target_vectorization([target_sentence])\n","        next_token_prediction = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n","        next_sample_token_index = np.argmax(next_token_prediction[0,i,:])\n","        next_sample_token = spanish_index_lookup[next_sample_token_index]\n","        target_sentence += \" \" + next_sample_token\n","        if next_sample_token == \"[end]\":\n","            break\n","    return target_sentence\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_eng_texts = [pair[0] for pair in test_samples]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{},"source":["Though an accuracy of 64 percent is claimed, upon fitting the model, only 28 percent accuracy is obtained. a deeper investigation is needed\n","and also, BLEU is a more reliable metric compared to accuracy for seq2seq translation."]},{"cell_type":"markdown","metadata":{},"source":["Drawbacks of rnn approach:\n","the entirety of source sequence representation is to be held in encoder -> very little flexibility especially when translating long ,complex sequences\n","rnns forget context with increase in number of tokens to remember. by the time we reach 100th token, the rnn has very little information about the zeroth token.thus it fails when we use it for long documents.\n","this paved way for transformer architecture [self attention + position embedding]"]},{"cell_type":"markdown","metadata":{},"source":["Transformer for sequence translation:\n","transformer decoder is very similar to transformer encoder except that there is a communication between exit block of transformer encoder and attention block of decoder.\n","\n","in a decoder, the queries are the target sentence representations while the source sequence representations are the keys an values, This way, for every token in sequence, there is a communication between target and source [unlike RNNs].\n","\n","Decoder : given.0...N tokens, predict the N+1 token."]},{"cell_type":"markdown","metadata":{},"source":["Transformer by default will look at all tokens in a sequence to calculate the key-pair scores and update value vector. but, during inference, you will have access to 0, n tokens and anything beyond. So , during training , if we have access to tokens beyond n, it willuse that info to have perfect training accuracy, but when used during inference, it will be useless spitting non-sense because it has been trained to predict given future tokens. So, causal mask [mask which filters out tokens N+1:end] will have to be added to architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n","        super().__init__(**kwargs)\n","        self.dense_dim = dense_dim\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.attention1 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n","        self.attention2 = layers.MultiHeadAttention(num_heads = num_heads,key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential([layers.Dense(dense_dim,activation='relu'), layers.Dense(embed_dim),])\n","        self.layer_norm1 = layers.LayerNormalization()\n","        self.layer_norm2 = layers.LayerNormalization()\n","        self.layer_norm3= layers.LayerNormalization()\n","        self.supports_masking = True\n","    \n","    def get_config(self):\n","        config = super().get_config\n","        config.update({\n","            \"dense_dim\":self.dense_dim,\n","            \"embed_dim\":self.embed_dim,\n","            \"num_heads\":self.num_heads\n","        })\n","        return config\n","        \n","        "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:14:09.902735Z","iopub.status.busy":"2024-04-28T12:14:09.902428Z","iopub.status.idle":"2024-04-28T12:14:10.262610Z","shell.execute_reply":"2024-04-28T12:14:10.261346Z","shell.execute_reply.started":"2024-04-28T12:14:09.902707Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'tf' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      4\u001b[0m seq_length_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mrange(sequence_length)[:, tf\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m j \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrange(sequence_length)\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}],"source":["#get causal attetion mask - understanding \n","#idea : mask one half of inputs so that future tokens are not seen during training\n","batch_size_causal = 64\n","seq_length_causal = 20\n","i = tf.range(sequence_length)[:, tf.newaxis]\n","print(i.shape)\n","j = tf.range(sequence_length)\n","mask = tf.cast(i >= j, dtype=\"int32\")\n","#print(mask)\n","mask = tf.reshape(mask, (1, seq_length_causal, seq_length_causal))\n","print(mask.shape)\n","print(tf.expand_dims(batch_size_causal, -1).shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
